{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45cbcc4c",
   "metadata": {},
   "source": [
    "## Homework 1\n",
    "\n",
    "### CS 490A, UMass CICS, Fall 2022\n",
    "### Due: September 21st at 11:59pm. Please see detailed submission instructions below.  70 points total.\n",
    "\n",
    "##### How to complete this homework:\n",
    "\n",
    "- Use Python version 3.  We strongly suggest installing Python from the [Anaconda Individual Edition](https://docs.anaconda.com/anaconda/) software package.\n",
    "\n",
    "- Add code or textual answers where prompted.\n",
    "\n",
    "- This assignment is designed so that you can run all cells in several minutes of computation time. If it is taking longer than that, you probably have a mistake in your code.\n",
    "\n",
    "##### How to submit this homework:\n",
    "- Write all the answers in this notebook. Once you are finished, (1) Generate a PDF via (File -> Download As -> PDF)  (2) Upload your PDF file to Gradescope. (3) Upload your notebook to Gradescope.\n",
    "\n",
    "- **Important:** Check your PDF before you upload it to Gradescope to make sure it exported correctly. If the notebook gets confused about your syntax, it will sometimes terminate the PDF creation routine early. You are responsible for checking for these errors. If your whole PDF does not print, try running on the commandline `jupyter nbconvert --to pdf hw1.ipynb` to identify and fix any syntax errors that might be causing problems.\n",
    "\n",
    "- **Important:** When creating your final version of the PDF, please do a fresh restart and execute every cell in order. Then you'll be sure it's actually right. One convenient way to do this is by clicking `Cell -> Run All` in the notebook menu.\n",
    "\n",
    "- If you are having trouble with PDF export, you can always paste screenshots into a word processor then turn that into PDF.\n",
    "\n",
    "\n",
    "##### Academic honesty:\n",
    "- Like always, check the course's collaboration / academic honesty policy that is available within the course syllabus (posted on Moodle). All of the content you submit for this homework, both code and text, must be your own independent work. Do not share code or written materials, and list any classmates that you had high-level discussions with. We will check your submissions for plagiarism, and we will follow up collaboration policy violations with the university’s Academic Honesty Policy and Procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4468d7",
   "metadata": {},
   "source": [
    "## Collaboration Declarations:\n",
    "List here the name of any classmates that you had high-level discussions with about the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2ca73e",
   "metadata": {},
   "source": [
    "**OPTIONAL: Your declarations here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b710cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1056)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to run this cell, it imports all the packages we will need for this assignment\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm # For progress bar in Part Two\n",
    "from nltk.tokenize import word_tokenize # For tokenization in Part Two\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab53ee0",
   "metadata": {},
   "source": [
    "## Part One: Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092fdf6e",
   "metadata": {},
   "source": [
    "### Regular Expressions\n",
    "\n",
    "Consider the following passage:\n",
    "$$\n",
    "\\text{\"It is very seldom,\" the young man said at last, \"that dragons ask to do men favors.\"}\n",
    "$$\n",
    "\n",
    "and the following regular expression:\n",
    "$$\n",
    "\\text{\".+\"}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b004d4f",
   "metadata": {},
   "source": [
    "**Question 1.1 (5 pts)** Describe what this regular expression will match on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dc59b6",
   "metadata": {},
   "source": [
    "**Your answer here**  \n",
    "The regular expression of \".+\" will match with one or more occurrences of any char except the new line character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94645c66",
   "metadata": {},
   "source": [
    "**Question 1.2 (5 pts)** List each of the matching substrings from the passage for this regular expression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cd944d",
   "metadata": {},
   "source": [
    "**Your answer here**   \n",
    "The matching substring from the passage will be every possible substring of the text and it can not be empty. \n",
    "Passage: \"It is very seldom,\" the young man said at last, \"that dragons ask to do men favors.\"\n",
    "So every Non-empty substring matches regular expression of \".+\"\n",
    "For example:  \n",
    "\"I\"  \n",
    "\"It\"  \n",
    "\"It \"  \n",
    "\"It i\"  \n",
    "\"It is\"  \n",
    "\"It is \"  \n",
    "This will go on till the last substring that will be equal to the complete passage.\n",
    "Also, other substrings such as \"is\", \"that\" and every possible Non-empty substring will match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1c5d89",
   "metadata": {},
   "source": [
    "Now, let's try using regular expressions in Python using the [`re`](https://docs.python.org/3/library/re.html) built-in package. The following method `get_matches` prints each match for a regular expression `regex` and input string `text` using [`re.findall`](https://docs.python.org/3/library/re.html#re.findall). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ef273e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches(regex, text):\n",
    "    matches = re.findall(regex, text)\n",
    "    print(\"Matches:\")\n",
    "    for i, match in enumerate(matches):\n",
    "        print(\"{}. {}\".format(i+1, match))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be81d2b",
   "metadata": {},
   "source": [
    "The following cell runs `get_matches` on our working passage and regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fae2495f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches:\n",
      "1. \"It is very seldom,\" the young man said at last, \"that dragons ask to do men favors.\"\n"
     ]
    }
   ],
   "source": [
    "passage = '\"It is very seldom,\" the young man said at last, \"that dragons ask to do men favors.\"'\n",
    "pattern = '\".+\"'\n",
    "\n",
    "get_matches(pattern, passage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a92f35",
   "metadata": {},
   "source": [
    "**Question 1.3 (5 pts)** Describe how the results from the previous code cell differ from your answer in **(b)**. Why are they different?\n",
    "\n",
    "*HINT: Take a look at this [page](https://docs.python.org/3/howto/regex.html#greedy-versus-non-greedy).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b863650",
   "metadata": {},
   "source": [
    "**Your answer here**  \n",
    "The result did not give all the possible matches for the regular expression \".+\".  \n",
    "They are different since the re.findall in get_matches would return all non-overlapping matches of pattern in string, as a list of strings. But in my answer in (b) there are lots of overlapping matches such as \"It\" and \"It is\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adcdec0",
   "metadata": {},
   "source": [
    "Now, let's try using a *non-greedy* version of the `+` operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85fa2f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches:\n",
      "1. \"It is very seldom,\"\n",
      "2. \"that dragons ask to do men favors.\"\n"
     ]
    }
   ],
   "source": [
    "passage = '\"It is very seldom,\" the young man said at last, \"that dragons ask to do men favors.\"'\n",
    "pattern = '\".+?\"'\n",
    "\n",
    "get_matches(pattern, passage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f509a8ff",
   "metadata": {},
   "source": [
    "**Question 1.4 (5 points)** Describe how these results differ from **1.2** and **1.3**. Why do these results differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd34f3a",
   "metadata": {},
   "source": [
    "**Your answer here**  \n",
    "For pattern \".+\", it is greedy and consumes as many characters as it can. However, for pattern \".+?\" is reluctant and consumes as few characters as it can."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b171e7",
   "metadata": {},
   "source": [
    "### Text Processing with Python\n",
    "Now, let's use regular expressions to extract and process text using Python. We'll be working with the following file `Avatar_2009_script.txt` which contains a script of James Cameron's 2009 film Avatar. The sequel, Avatar 2, is expected to release this year, but it's been quite a few years since the original film came out, so let's try using text processing to learn a bit more about the original movie.\n",
    "\n",
    "We'll focus on learning about the characters of this film and how often they have speaking lines within the script. We can use the *structure* of the text file to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5aa7a1",
   "metadata": {},
   "source": [
    "In the following excerpt, the character speaking lines are *italicized* and the characters speaking these lines is **bolded**. For visualization purposes spaces are being represented as \"·\"s.\n",
    "\n",
    "<pre>\n",
    "··········Some of the other scientists clap Jake on the shoulders in\n",
    "··········congratulation.\n",
    "··········\n",
    "······························<b>MAX</b>\n",
    "····················<i>That's awesome, Jake.</i>\n",
    "··········\n",
    "··········NORM chomps his bacon, fuming.\n",
    "··········\n",
    "······························<u>·</u><b>GRACE</b>\n",
    "·························(to Jake, getting serious)\n",
    "····················<i>For reasons I cannot fathom, the</i>\n",
    "····················<i>Omaticaya have chosen you. God help us</i>\n",
    "····················<i>all.</i>\n",
    "·································································CUT TO:\n",
    "··········\n",
    "··········INT. OPS CENTER - MORNING\n",
    "</pre>\n",
    "\n",
    "*Note that this structure isn't perfect. There are inconsistencies, one is <u>underlined</u> above.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea45fe36",
   "metadata": {},
   "source": [
    "**Question 1.5 (5 points)** Examine the structure of the script and describe how *dialogue* is organized within the script. How are the lines of spoken dialogue organized? How are the character names assigned for each line of dialogue organized?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff44422a",
   "metadata": {},
   "source": [
    "**Your answer here**  \n",
    "For dialogue, it is first with name of who is gonna speak which is in bold and then probablily also a () with a little explaination in it about who is the character speaking to and under what situation.   \n",
    "The character names is after 30 space ; () with explaination is after 25 space ; dialogue is after 20 space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabf3d6a",
   "metadata": {},
   "source": [
    "**Question 1.6 (5 points)** Modify the regular expression `pattern` in the following code cell so that it will only match on the character name that corresponds to each instance of dialogue. We'll accumulate these matches into a [`Counter`](https://docs.python.org/3/library/collections.html#collections.Counter) `character_counts`.\n",
    "\n",
    "For example, the following excerpt will result in a single match `\"MAX\"`.\n",
    "\n",
    "<pre>\n",
    "······························MAX\n",
    "····················That's awesome, Jake.\n",
    "</pre>\n",
    "\n",
    "*Note that since this structure isn't perfect, make a best effort at extracting character names.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e37d1934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['AGENT']\n",
      "['AGENT']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['AGENT']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['MED']\n",
      "['MED']\n",
      "['CREW']\n",
      "['CREW']\n",
      "['CREW']\n",
      "['CREW']\n",
      "['WAINFLEET']\n",
      "['FIKE']\n",
      "['WAINFLEET']\n",
      "['JAKE']\n",
      "['WAINFLEET']\n",
      "['MAN']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['JAKE']\n",
      "['NORM']\n",
      "['NORM']\n",
      "['NORM']\n",
      "['JAKE']\n",
      "['NORM']\n",
      "['MAX']\n",
      "['NORM']\n",
      "['MAX']\n",
      "['JAKE']\n",
      "['NORM']\n",
      "['JAKE']\n",
      "['NORM']\n",
      "['JAKE']\n",
      "['MAX']\n",
      "['NORM']\n",
      "['MAX']\n",
      "['GRACE']\n",
      "['MAX']\n",
      "['GRACE']\n",
      "['NORM']\n",
      "['GRACE']\n",
      "['NORM']\n",
      "['MAX']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['MAX']\n",
      "['MAX']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['SELFRIDGE']\n",
      "['SELFRIDGE']\n",
      "['GRACE']\n",
      "['SELFRIDGE']\n",
      "['SELFRIDGE']\n",
      "['GRACE']\n",
      "['SELFRIDGE']\n",
      "['GRACE']\n",
      "['NORM']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['MAX']\n",
      "['MAX']\n",
      "['TECH']\n",
      "['MAX']\n",
      "['MED']\n",
      "['MAX']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['NORM']\n",
      "['NORM']\n",
      "['QUARITCH']\n",
      "['TROOPERS']\n",
      "['QUARITCH']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['TRUDY']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['JAKE']\n",
      "['MECHANIC']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['JAKE']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['NORM']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['TRUDY']\n",
      "['NORM']\n",
      "['GRACE']\n",
      "['WAINFLEET']\n",
      "['WAINFLEET']\n",
      "['GRACE']\n",
      "['NORM']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['NORM']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['TRUDY']\n",
      "['GRACE']\n",
      "['TRUDY']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['TSU']\n",
      "['NEYTIRI']\n",
      "['EYTUKAN']\n",
      "['NEYTIRI']\n",
      "['EYTUKAN']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['MO']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['MO']\n",
      "['JAKE']\n",
      "['MO']\n",
      "['JAKE']\n",
      "['MO']\n",
      "['JAKE']\n",
      "['MO']\n",
      "['JAKE']\n",
      "['TSU']\n",
      "['EYTUKAN']\n",
      "['JAKE']\n",
      "['MO']\n",
      "['NEYTIRI']\n",
      "['MO']\n",
      "['MO']\n",
      "['MO']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['TSU']\n",
      "['MO']\n",
      "['JAKE']\n",
      "['MO']\n",
      "['EYTUKAN']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['TSU']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['MAX']\n",
      "['QUARITCH']\n",
      "['JAKE']\n",
      "['QUARITCH']\n",
      "['SELFRIDGE']\n",
      "['SELFRIDGE']\n",
      "['SELFRIDGE']\n",
      "['JAKE']\n",
      "['SELFRIDGE']\n",
      "['JAKE']\n",
      "['QUARITCH']\n",
      "['JAKE']\n",
      "['QUARITCH']\n",
      "['SELFRIDGE']\n",
      "['QUARITCH']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['NORM']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['MAX']\n",
      "['GRACE']\n",
      "['NEYTIRI']\n",
      "['NEYTIRI']\n",
      "['TSU']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['QUARITCH']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['NORM']\n",
      "['GRACE']\n",
      "['NORM']\n",
      "['TRUDY']\n",
      "['TRUDY']\n",
      "['GRACE']\n",
      "['TRUDY']\n",
      "['GRACE']\n",
      "['NORM']\n",
      "['TRUDY']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['NORM']\n",
      "['GRACE']\n",
      "['NORM']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['NEYTIRI']\n",
      "['NEYTIRI']\n",
      "['NEYTIRI']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['NORM']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['TSU']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['TRUDY']\n",
      "['JAKE']\n",
      "['NORM']\n",
      "['JAKE']\n",
      "['TRUDY']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['TRUDY']\n",
      "['TRUDY']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['TSU']\n",
      "['JAKE']\n",
      "['TSU']\n",
      "['NEYTIRI']\n",
      "['MO']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['SELFRIDGE']\n",
      "['SELFRIDGE']\n",
      "['SELFRIDGE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['SELFRIDGE']\n",
      "['GRACE']\n",
      "['SELFRIDGE']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['QUARITCH']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['NORM']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['NORM']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['NORM']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['NORM']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['NEYTIRI']\n",
      "['MO']\n",
      "['MO']\n",
      "['EYTUKAN']\n",
      "['MO']\n",
      "['EYTUKAN']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['NEYTIRI']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['NEYTIRI']\n",
      "['NEYTIRI']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['OPERATOR']\n",
      "['SUPERVISOR']\n",
      "['SELFRIDGE']\n",
      "['QUARITCH']\n",
      "['SELFRIDGE']\n",
      "['QUARITCH']\n",
      "['EYTUKAN']\n",
      "['GRACE']\n",
      "['TSU']\n",
      "['JAKE']\n",
      "['TSU']\n",
      "['MO']\n",
      "['NEYTIRI']\n",
      "['TSU']\n",
      "['TSU']\n",
      "['MO']\n",
      "['NEYTIRI']\n",
      "['EYTUKAN']\n",
      "['TSU']\n",
      "['GRACE']\n",
      "['TSU']\n",
      "['QUARITCH']\n",
      "['GRACE']\n",
      "['QUARITCH']\n",
      "['GRACE']\n",
      "['SELFRIDGE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['SELFRIDGE']\n",
      "['GRACE']\n",
      "['SELFRIDGE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['SELFRIDGE']\n",
      "['SELFRIDGE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['QUARITCH']\n",
      "['GRACE']\n",
      "['SELFRIDGE']\n",
      "['WAINFLEET']\n",
      "['QUARITCH']\n",
      "['WAINFLEET']\n",
      "['SELFRIDGE']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['SELFRIDGE']\n",
      "['GRACE']\n",
      "['NORM']\n",
      "['TRUDY']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['SELFRIDGE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['SELFRIDGE']\n",
      "['JAKE']\n",
      "['EYTUKAN']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['MO']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['JAKE']\n",
      "['EYTUKAN']\n",
      "['QUARITCH']\n",
      "['EYTUKAN']\n",
      "['MO']\n",
      "['JAKE']\n",
      "['QUARITCH']\n",
      "['PILOTS']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['EYTUKAN']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['SELFRIDGE']\n",
      "['GRACE']\n",
      "['TRUDY']\n",
      "['GUARD']\n",
      "['TRUDY']\n",
      "['MAX']\n",
      "['NORM']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['TRUDY']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['JAKE']\n",
      "['MO']\n",
      "['NORM']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['NEYTIRI']\n",
      "['CROWD']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['TSU']\n",
      "['JAKE']\n",
      "['MO']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['MO']\n",
      "['MO']\n",
      "['JAKE']\n",
      "['MO']\n",
      "['JAKE']\n",
      "['GRACE']\n",
      "['GRACE']\n",
      "['MO']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['SELFRIDGE']\n",
      "['SELFRIDGE']\n",
      "['QUARITCH']\n",
      "['SELFRIDGE']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['SELFRIDGE']\n",
      "['MAX']\n",
      "['MAX']\n",
      "['TRUDY']\n",
      "['JAKE']\n",
      "['MAX']\n",
      "['MAX']\n",
      "['NORM']\n",
      "['TRUDY']\n",
      "['JAKE']\n",
      "['TRUDY']\n",
      "['NORM']\n",
      "['JAKE']\n",
      "['TRUDY']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['TRUDY']\n",
      "['JAKE']\n",
      "['TRUDY']\n",
      "['JAKE']\n",
      "['TRUDY']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['NEYTIRI']\n",
      "['JAKE']\n",
      "['WAINFLEET']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['TRUDY']\n",
      "['QUARITCH']\n",
      "['TROOPER']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['WAINFLEET']\n",
      "['TROOPER']\n",
      "['WAINFLEET']\n",
      "['PILOT']\n",
      "['QUARITCH']\n",
      "['SELFRIDGE']\n",
      "['PILOT']\n",
      "['QUARITCH']\n",
      "['QUARITCH']\n",
      "['JAKE']\n",
      "['TRUDY']\n",
      "['NORM']\n",
      "['TRUDY']\n",
      "['JAKE']\n",
      "['QUARITCH']\n",
      "['JAKE']\n",
      "['QUARITCH']\n",
      "['TSU']\n",
      "['JAKE']\n",
      "['TSU']\n",
      "['JAKE']\n",
      "['JAKE']\n",
      "['TSU']\n",
      "['JAKE']\n",
      "['TSU']\n",
      "['JAKE']\n",
      "['TSU']\n",
      "['TSU']\n",
      "['JAKE']\n",
      "['JAKE']\n"
     ]
    }
   ],
   "source": [
    "pattern = \"^[ ]{30}([A-Z]{1,10})\"  # Modify this pattern \"\\w+S+\"\n",
    "\n",
    "character_counts = Counter()\n",
    "with open(\"Avatar_2009_script.txt\", encoding=\"utf-8\") as reader:\n",
    "    for line in reader:\n",
    "        matches = re.findall(pattern, line)\n",
    "        if matches:\n",
    "            print(matches)\n",
    "            character_counts.update(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e1f99",
   "metadata": {},
   "source": [
    "**Question 1.7 (5 points)** Now let's use `character_counts` so we can examine the dialogue counts for characters in Avatar.\n",
    "\n",
    "Write code in the following cell, so that it prints a line for each character containing the character's name and their dialogue count. These lines must be *sorted* in descending order by dialogue counts.\n",
    "\n",
    "```\n",
    "[Character] [Highest Dialogue Count]\n",
    "[Character] [Second Highest Dialogue Count]\n",
    ".\n",
    ".\n",
    ".\n",
    "[Character] [Lowest Dialogue Count]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c221e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[JAKE] [210]\n",
      "[GRACE] [113]\n",
      "[NEYTIRI] [63]\n",
      "[QUARITCH] [54]\n",
      "[NORM] [35]\n",
      "[SELFRIDGE] [33]\n",
      "[TRUDY] [26]\n",
      "[MO] [25]\n",
      "[TSU] [21]\n",
      "[MAX] [19]\n",
      "[EYTUKAN] [12]\n",
      "[WAINFLEET] [10]\n",
      "[CREW] [4]\n",
      "[AGENT] [3]\n",
      "[MED] [3]\n",
      "[TROOPER] [2]\n",
      "[PILOT] [2]\n",
      "[FIKE] [1]\n",
      "[MAN] [1]\n",
      "[TECH] [1]\n",
      "[TROOPERS] [1]\n",
      "[MECHANIC] [1]\n",
      "[OPERATOR] [1]\n",
      "[SUPERVISOR] [1]\n",
      "[PILOTS] [1]\n",
      "[GUARD] [1]\n",
      "[CROWD] [1]\n"
     ]
    }
   ],
   "source": [
    "# Write code here\n",
    "temp = character_counts.most_common()\n",
    "for key in temp:\n",
    "    print('[' + key[0] + '] [' + str(key[1]) +']')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66acb7fb",
   "metadata": {},
   "source": [
    "**Question 1.8 (5 points)** What does this output tell us about Avatar? If you haven't seen the film or don't remember it, feel free to refer to [Wikipedia](https://en.wikipedia.org/wiki/Avatar_(2009_film)) and [IMDB](https://www.imdb.com/title/tt0499549/). Are the characters with the most spoken dialogue who you expect? Are there characters that have surprisingly little dialogue? Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa4f49",
   "metadata": {},
   "source": [
    "**Your answer here**  \n",
    "This output tells me that 'Jake' is the male protagonist, and 'Grac' is the female protagonist. And other charaters such as 'Neytiri' and 'Quaritch' and 'Norm' and 'Selfridge' and others are also characters in Avatar.  \n",
    "Jake speak most dialogue and this is also what I am expecting.  \n",
    "I am sprsingling see little dialogue for guard and crowd since they seem to speak may be more than once in Avatar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94212823",
   "metadata": {},
   "source": [
    "## Part Two: Language Modeling\n",
    "In this part we will implement several n-gram language models. We will train and evaluate these language models using a [collection of movie summaries](http://www.cs.cmu.edu/~ark/personas/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b75f4b",
   "metadata": {},
   "source": [
    "### Setup: Building working datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "58f4b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens(text):\n",
    "    tokens = [\"<bos>\", \"<bos>\"]\n",
    "    clean_text = \" \".join(text.strip().lower().split())\n",
    "    tokens.extend(word_tokenize(clean_text))\n",
    "    tokens.append(\"<eos>\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "708cfd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61824780a6454ad68f44890ec06ce4c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building test set\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df684b33babd42378dbeccbabe69d004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 116191 word types\n",
      "Example text: ['<bos>', '<bos>', 'shlykov', ',', 'a', 'hard-working', 'taxi', 'driver', 'and', 'lyosha', ',', 'a', 'saxophonist', ',', 'develop', 'a', 'bizarre', 'love-hate', 'relationship', ',']\n"
     ]
    }
   ],
   "source": [
    "# Decide train and test set size (in number of plot summaries)\n",
    "train_size = 20000\n",
    "test_size = 5000\n",
    "\n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "with open('plot_summaries.txt', encoding='utf-8') as reader:\n",
    "    # Extract tokens for training set\n",
    "    print(\"Building training set\")\n",
    "    for i in tqdm(range(train_size)):\n",
    "        line = reader.readline()\n",
    "        summary_text = line.split('\\t')[1]\n",
    "        train_set.extend(extract_tokens(summary_text))\n",
    "    # Extract tokens for test set\n",
    "    print(\"Building test set\")\n",
    "    for i in tqdm(range(test_size)):\n",
    "        line = reader.readline()\n",
    "        summary_text = line.split('\\t')[1]\n",
    "        test_set.extend(extract_tokens(summary_text))\n",
    "\n",
    "# Vocabulary information\n",
    "word_counts = Counter(train_set)\n",
    "vocab = sorted(word_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Vocabulary size: {} word types\".format(vocab_size))\n",
    "print(\"Example text: {}\".format(train_set[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb55aa",
   "metadata": {},
   "source": [
    "### N-gram Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f650d3",
   "metadata": {},
   "source": [
    "First, let's build code to build n-gram models. To train these models, we will use the n-gram (and n-1 gram) counts from our training set to compute the relative frequencies of n-grams. To ensure that no word in the vocabulary has $0$ probability, we'll use additive smoothing. More formally, we'll compute the probability of the next word as follows:\n",
    "\n",
    "$$\\Pr(w_n \\mid w_1 \\dots w_{n-1}) = \n",
    "\\frac{C(w_1 \\dots w_n) + \\alpha}{C(w_1 \\dots w_{n-1}) + \\alpha \\cdot |V|}$$\n",
    "\n",
    "where $C$ is the count for a given n-gram (in the training set) and $|V|$ is the size of the working vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c6e82",
   "metadata": {},
   "source": [
    "**Question 2.1 (10 points)** The following code cell contains a partial implementation of a class for n-gram models, `NGramModel`. Implement its `get_final_word_prob` method. It computes the probability for the final word of the input text, `tokens` which is a list of string tokens.\n",
    "\n",
    "- *Hints:* \n",
    "- Make sure to use the equation listed above\n",
    "- `self.n_counts` contains the observed counts for `self.n` grams\n",
    "- `self.n_1_counts` contains the observed counts for `self.n-1` grams\n",
    "- Use the [`join`](https://docs.python.org/3/library/stdtypes.html#str.join) to create a string from a list of strings\n",
    "- `self.vocab` contains a list of the vocabulary\n",
    "- Remember that input text can be larger than `self.n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "be6ce7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "    # Initialize n-gram model\n",
    "    def __init__(self, train_tokens, vocab, n, alpha=1e-3):\n",
    "        self.n = n\n",
    "        self.smoothing = alpha\n",
    "        self.vocab = vocab\n",
    "        \n",
    "        # Gather n-gram counts\n",
    "        self.n_counts = Counter()\n",
    "        current_tokens = [\"<bos>\"] * (self.n - 1) # Padding for initial word\n",
    "        for token in train_tokens:\n",
    "            current_tokens.append(token) # Add next token\n",
    "            n_gram = \" \".join(current_tokens)\n",
    "            self.n_counts[n_gram] += 1\n",
    "            current_tokens.pop(0) # Remove first token\n",
    "        \n",
    "        #print(self.n_counts)\n",
    "        \n",
    "        # Gather n-1 gram counts\n",
    "        self.n_1_counts = Counter()\n",
    "        for n_gram, count in self.n_counts.items():\n",
    "            n_1_gram = \" \".join(n_gram.split()[:-1])\n",
    "            self.n_1_counts[n_1_gram] += count\n",
    "            \n",
    "        #print(self.n_1_counts)\n",
    "    \n",
    "    # Return the probability for the final word of an n-gram.\n",
    "    def get_final_word_prob(self, tokens):\n",
    "        assert len(tokens) >= self.n, \"ERROR: Input too short. Expected {}+ tokens, but received {}.\".format(\n",
    "            self.n, len(tokens))\n",
    "        # WRITE YOUR CODE HERE\n",
    "        lamda = self.smoothing\n",
    "        tokenList = tokens.split();\n",
    "        if len(tokenList[-1]) <2 :\n",
    "            word = tokenList[-2]\n",
    "            n_before = \" \".join(tokenList[-2-self.n:-2])\n",
    "        else:\n",
    "            n_before = \" \".join(tokenList[-1-self.n:-1])\n",
    "            word = tokenList[-1]\n",
    "        if self.n == 1:\n",
    "            word = ''\n",
    "        #print('aaa')\n",
    "        #print(tokens)\n",
    "        #print(word)\n",
    "        #print(n_before)\n",
    "        #print(self.n_counts[n_before])\n",
    "        #print(self.n_1_counts[word])\n",
    "        return (self.n_counts[n_before]+lamda)/(self.n_1_counts[word]+lamda*len(self.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1352c6b7",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d15b45d",
   "metadata": {},
   "source": [
    "To evaluate the quality of our language models, we use an *intrinsic* evaluation metric called perplexity.\n",
    "The perplexity (PP) on a test set is the inverse probability of the test set normalized by the number of words.\n",
    "For a test set $W=w_1w_2\\dots w_N$, perplexity is defined as follows:\n",
    "\n",
    "$$PP(W)=\\Pr(w_1w_2\\dots w_N)^{-\\frac{1}{N}} = \\sqrt[N]{\\frac{1}{\\Pr(w_1w_2\\dots w_N)}}$$\n",
    "\n",
    "Using the chain rule we can reformulate perplexity as follows:\n",
    "$$PP(W) = \\sqrt[N]{\\frac{1}{\\prod_{i=1}^N\\Pr(w_i \\mid w_1 \\dots w_{i-1})}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a12bc",
   "metadata": {},
   "source": [
    "Since we'll be multiplying quite a few probabilities (many of which are very small), we'll first compute values in the log domain to avoid numerical underflow. Then, we can compute our final perplexity score by exponentiating our log perplexity score. As a result, we'll use the following two equations:\n",
    "\n",
    "$$\\ln PP(W) = -\\frac{1}{N}\\sum_{i=1}^{N}\\ln\\Pr(w_i \\mid w_1 \\dots w_{i-1})$$\n",
    "\n",
    "$$PP(W) = e^{\\ln PP(W)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae28a5",
   "metadata": {},
   "source": [
    "**Question 2.2 (10 pts)** Implement the following method `get_perplexity` which takes an NGramModel `model` and test set `test_tokens` (list of string tokens) as input, and returns the perplexity score of `model` for `test_tokens`.\n",
    "\n",
    "Hints:\n",
    "- Your implementation will use `model.get_probability`\n",
    "- Your implementation should use `math.log` and `math.exp`\n",
    "- Remember to add padding (use `<bos>` tokens) for the first n-1 tokens in the test set\n",
    "- You will need to compute the probability of each successive n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "392bcb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity(model, test_tokens):\n",
    "    # WRITE YOUR CODE HERE\n",
    "    prob = 0.0\n",
    "    count = 0\n",
    "    i = 0\n",
    "    str_temp = ''\n",
    "    while i < len(test_tokens):\n",
    "        if test_tokens[i] == '<bos>':\n",
    "            i+=1\n",
    "        elif test_tokens[i] == '<eos>':\n",
    "            prob += math.log(model.get_final_word_prob(str_temp))\n",
    "            #print(model.get_final_word_prob(str_temp))\n",
    "            i+=1\n",
    "            count+=1\n",
    "            str_temp = ''\n",
    "        else:\n",
    "            str_temp += ' '+test_tokens[i]\n",
    "            i+=1\n",
    "    help_p = (-float(1)/count)*prob \n",
    "    return math.exp(help_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1200c1",
   "metadata": {},
   "source": [
    "Now, let's train unigram, bigram, and trigram models using our training set `train_set` and measure the perplexity score of each model for our test set `test_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "fc2b5862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram test perplexity: 1368.3\n",
      "\n",
      "bigram test perplexity: 67.0\n",
      "\n",
      "trigram test perplexity: 999.779008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unigram_model = NGramModel(train_set, vocab, n=1)\n",
    "\n",
    "# This score should be around 1300\n",
    "print('unigram test perplexity: {:.1f}\\n'.format(get_perplexity(unigram_model, test_set)))\n",
    "\n",
    "# This score should be around 600\n",
    "bigram_model = NGramModel(train_set, vocab, n=2)\n",
    "print('bigram test perplexity: {:.1f}\\n'.format(get_perplexity(bigram_model, test_set)))\n",
    "\n",
    "# This score should be around 3300-3400\n",
    "trigram_model = NGramModel(train_set, vocab, n=3)\n",
    "print('trigram test perplexity: {:1f}\\n'.format(get_perplexity(trigram_model, test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bec587",
   "metadata": {},
   "source": [
    "**Question 2.3 (5 pts)** Why does the trigram model perform so poorly (i.e. have high perplexity)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152f013",
   "metadata": {},
   "source": [
    "**Your answer here**  \n",
    "Since our |v| = 116191, in this case, with lower n value such as 1,2,3, it is hard to predict the letter after n letter before with this high vocab which result high prelexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0679876c",
   "metadata": {},
   "source": [
    "What if we instead measured the perplexity of our trained models for our training set `train_set`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "42b761f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram test perplexity: 1127.9\n",
      "\n",
      "bigram test perplexity: 18.9\n",
      "\n",
      "trigram test perplexity: 28.570622\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('unigram test perplexity: {:.1f}\\n'.format(get_perplexity(unigram_model, train_set)))\n",
    "\n",
    "bigram_model = NGramModel(train_set, vocab, n=2)\n",
    "print('bigram test perplexity: {:.1f}\\n'.format(get_perplexity(bigram_model, train_set)))\n",
    "\n",
    "trigram_model = NGramModel(train_set, vocab, n=3)\n",
    "print('trigram test perplexity: {:1f}\\n'.format(get_perplexity(trigram_model, train_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcae404",
   "metadata": {},
   "source": [
    "**Question 2.4 (5 points)** How do these perplexity scores and their orderings differ from those of **2.3**? Is this surprising? Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9329e9e",
   "metadata": {},
   "source": [
    "**Your answer here**  \n",
    "All preplexity scores are lower than the value in test case. This is not surprising since we are trarning the model by these training set which means we are gonna predict these value more accurate than test case since we trained by it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538a353c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
