{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3: Text Classification\n",
    "\n",
    "### CS 490A, UMass CICS, Fall 2022\n",
    "### Due: October 5th at 11:59pm. 95 points total.\n",
    "\n",
    "In this homework, you will design features for and use Logistic Regression models to classify a collection of movie reviews in order to distinguish between positive and negative reviews.\n",
    "\n",
    "##### How to submit this homework:\n",
    "Write all the answers in this notebook. Once you are finished, you must submit the following to Gradescope:\n",
    "1. The generated PDF of your completed notebook\n",
    "2. Your completed notebook `hw3.ipynb` along with your predictions file `final_predictions.tsv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaboration Declarations:\n",
    "List here the name of any classmates that you had high-level discussions with about the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIONAL: Your declarations here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1056)>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "nltk.download('punkt')\n",
    "sns.set(style=\"white\", font_scale=1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation\n",
    "\n",
    "We'll use the following class `LogisticRegression` in this homework assignment. It relies on the scikit-learn implementation. **Do not modify this code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "#   DO NOT MODIFY!   #\n",
    "######################\n",
    "class LogisticRegression:\n",
    "    \"\"\"A logistic regression model for text classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, featurizer_method, min_feature_df=1,\n",
    "                 regularization_inverse_strength=1.0, \n",
    "                 ):\n",
    "        self.feature_index = {}\n",
    "        self.featurizer_method = featurizer_method\n",
    "        self.min_feature_df = min_feature_df\n",
    "        \n",
    "        self.model = None\n",
    "        self.regularization_inverse_strength = regularization_inverse_strength\n",
    "    \n",
    "    \n",
    "    def load_data(self, filename):\n",
    "        \"\"\"\n",
    "        This function loads the labels and texts from the input file.\n",
    "        Texts are converted into bags of features.\n",
    "        \"\"\"\n",
    "        bags_of_feats = []\n",
    "        labels = []\n",
    "        with open(filename, encoding=\"utf-8\") as reader:\n",
    "            for line in reader:\n",
    "                fields = line.strip().split('\\t')\n",
    "                label = fields[1]\n",
    "                text = fields[2]\n",
    "                labels.append(label)\n",
    "                bags_of_feats.append(self.featurizer_method(text))\n",
    "        return bags_of_feats, labels\n",
    "\n",
    "    \n",
    "    def build_feature_index(self, training_data):\n",
    "        \"\"\"\n",
    "        This function builds the working feature set from the input\n",
    "        training data and an index over these features (i.e. a mapping\n",
    "        from features to their numeric indices)\n",
    "        \"\"\"\n",
    "        # Exit early if feature_index already exists \n",
    "        if self.feature_index:\n",
    "            print(\"WARNING: Feature index already exists\")\n",
    "            return\n",
    "        \n",
    "        # Collect the document frequencies for each feature\n",
    "        feature_dfs = Counter()\n",
    "        for bag_of_feats in training_data:\n",
    "            feature_dfs.update(bag_of_feats.keys())\n",
    "        # Build index\n",
    "        feat_id = 0\n",
    "        for feat in feature_dfs:\n",
    "            if feature_dfs[feat] >= self.min_feature_df:\n",
    "                self.feature_index[feat] = feat_id\n",
    "                feat_id += 1\n",
    "\n",
    "    \n",
    "    def load_and_process_data(self, filename, isTraining=False):\n",
    "        \"\"\"\n",
    "        This function loads and processes data from the given file.\n",
    "        It returns the data in a form suitable as input for the underlying\n",
    "        scikit-learn logistic regression model (self.model).\n",
    "        \"\"\"\n",
    "        bags_of_feats, labels = self.load_data(filename)\n",
    "\n",
    "        # Build feature index if loading training data\n",
    "        if isTraining:\n",
    "            self.build_feature_index(bags_of_feats)\n",
    "        \n",
    "        assert self.feature_index, \"ERROR: Model has no working features\"\n",
    "        \n",
    "        # Vectorize bags of features \n",
    "        F = len(self.feature_index)\n",
    "        N = len(bags_of_feats)\n",
    "        X = lil_matrix((N, F))\n",
    "        for i, bag_of_feats in enumerate(bags_of_feats):\n",
    "            for feat, value in bag_of_feats.items():\n",
    "                if feat in self.feature_index:\n",
    "                    feat_idx = self.feature_index[feat]\n",
    "                    X[i, feat_idx] = value\n",
    "        return X, labels\n",
    "\n",
    "    \n",
    "    def train_model(self, filename):\n",
    "        \"\"\"\n",
    "        This function processes the input training file and uses\n",
    "        this data to train the logistic regression model.\n",
    "        \"\"\"\n",
    "        X, Y = self.load_and_process_data(filename, isTraining=True)\n",
    "        \n",
    "        self.model = sklearn.linear_model.LogisticRegression(\n",
    "                        penalty=\"l2\",\n",
    "                        C=self.regularization_inverse_strength,\n",
    "                        solver='liblinear'\n",
    "                        )\n",
    "        self.model.fit(X, Y)\n",
    "    \n",
    "    \n",
    "    def print_weights(self, display_k=5):\n",
    "        \"\"\"\n",
    "        This function prints the top feature weights for each label class.\n",
    "        \"\"\"\n",
    "        assert self.model, \"ERROR: Must train model first\"\n",
    "\n",
    "        reverse_feature_idx = {idx:feat for feat, idx in self.feature_index.items()}        \n",
    "        \n",
    "        weights = self.model.coef_[0]\n",
    "        sorted_idx = np.argsort(weights)\n",
    "        \n",
    "        label = self.model.classes_[1]\n",
    "        for i in sorted_idx[-display_k:][::-1]:\n",
    "            weight = weights[i]\n",
    "            if weight <= 0:\n",
    "                continue\n",
    "            feat = reverse_feature_idx[i]\n",
    "            print(\"{}\\t{}\\t{:.2f}\".format(label, feat, weight))\n",
    "        print()\n",
    "        \n",
    "        label = self.model.classes_[0]\n",
    "        for i in sorted_idx[:display_k]:\n",
    "            weight = weights[i]\n",
    "            if weight >= 0:\n",
    "                continue\n",
    "            feat = reverse_feature_idx[i]\n",
    "            print(\"{}\\t{}\\t{:.2f}\".format(label, feat, weight))\n",
    "        \n",
    "    \n",
    "    def evaluate_classifier_accuracy(self, filename):\n",
    "        \"\"\"\n",
    "        This function computes the model accuracy for the data in the input file.\n",
    "        \"\"\"\n",
    "        assert self.model, \"ERROR: Must train model first\"\n",
    "        \n",
    "        X, Y = self.load_and_process_data(filename)\n",
    "        accuracy = self.model.score(X, Y)\n",
    "        return accuracy\n",
    "        \n",
    "    \n",
    "    def predict(self, filename, includeClassProbs=False):\n",
    "        \"\"\"\n",
    "        Returns the model predictions for the data in the input file\n",
    "        and optionally the estimated class probabilities\n",
    "        \"\"\"\n",
    "        assert self.model, \"ERROR: Must train model first\"\n",
    "        \n",
    "        X, Y = self.load_and_process_data(filename)\n",
    "        predictions = self.model.predict(X)\n",
    "        \n",
    "        # Optionally return the estimated class probabilities too\n",
    "        if includeClassProbs:\n",
    "            probs = self.model.predict_proba(X)\n",
    "            return predictions, probs\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Getting Started (15 pts)\n",
    "Since Logisitic Regression does not require its *features* to be independent (unlike Naive Bayes), our working features for text classification can go well beyond word counts. In this assignment, we will transform our texts into *bags of features* which we will use as input for our classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, let's use simple dictionary-based features. If a review contains the word *love*, *great*, or *fantastic*, then we'll set the `contains_positive_term` feature to 1. Similarly, if a review contains the word *hate*, *terrible*, or *dreadful*, then we'll set the `contains_negative_term` feature to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_featurizer(text):\n",
    "    feats = {}\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in [\"love\", \"great\", \"fantastic\"]:\n",
    "            feats[\"contains_positive_term\"] = 1\n",
    "        if token in [\"hate\", \"terrible\", \"dreadful\"]:\n",
    "            feats[\"contains_negative_term\"] = 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a method for converting texts to bags of features, we can build a classifier using the provided `LogisticRegression` class.\n",
    "\n",
    "Similar to Homework 2's `NaiveBayes` class, we must initialize our classifier before we can train it. For initialization, the `LogisticRegression` class requires a method for converting texts into bags of features. This class has several *optional* inputs, we'll cover them later on in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our `simple_featurizer` featurizer to create a Logistic Regression classifier and train it using the training data in `train.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_classifier = LogisticRegression(simple_featurizer)\n",
    "simple_classifier.train_model(\"train.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then evaluate this classifier by computing the accuracy for our training set and our validation set in `dev.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.624\n",
      "Validation Accuracy: 0.610\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy: {:.3f}\".format(simple_classifier.evaluate_classifier_accuracy(\"train.tsv\")))\n",
    "print(\"Validation Accuracy: {:.3f}\".format(simple_classifier.evaluate_classifier_accuracy(\"dev.tsv\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided the following method `evaluate_classifier` to output several useful properties of a *trained* classifier:\n",
    "- the name of its featurizer method\n",
    "- the number of working features\n",
    "- its accuracy for our training set\n",
    "- its accuracy for our validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "#   DO NOT MODIFY!   #\n",
    "######################\n",
    "def evaluate_classifier(classifier):\n",
    "    \"\"\"\n",
    "    Evaluates a trained classifier\n",
    "    \"\"\"\n",
    "    train_acc = classifier.evaluate_classifier_accuracy(\"train.tsv\")\n",
    "    dev_acc = classifier.evaluate_classifier_accuracy(\"dev.tsv\")\n",
    "    featurizer_name = classifier.featurizer_method.__name__\n",
    "    n_features = len(classifier.feature_index)\n",
    "    print(\"Featurizer: {}, # Features: {}\".format(featurizer_name, n_features))\n",
    "    print(\"Training Accuracy: {:.1f}%, Validation Accuracy: {:.1f}%\".format(train_acc*100 ,dev_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizer: simple_featurizer, # Features: 2\n",
      "Training Accuracy: 62.4%, Validation Accuracy: 61.0%\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(simple_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although an accuracy of 60% does not seem particularly high, it's worth comparing this performance with a simple baseline, namely the \"choose majority\" classifier. As the name suggests, this classifier once trained returns a single label no matter the input. This label is chosen by determining the most frequent label class according to the labels seen during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1 (5 pts)**\n",
    "\n",
    "Take a look at the labels of our training and validation sets (i.e., the second column of each .tsv). Assume we train the \"choose majority\" classifier using our training set, what will be this classifier's accuracy for both the training and validation sets? How does it compare to the accuracies we got using `simple_featurizer`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**  \n",
    "If we train the \"choose majority\", our model will be more likely to predict the majority case will predicting testing set. In this way, the biased model will have a lower perform.  \n",
    "For comparing to the accuracies, it will have a higher training accuracy but lower validation accuracy while testing by a 50/50 labled test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LogisticRegression` class has a method `print_weights` that prints the top weights for each of our classes: *pos* and *neg*. The optional parameter `display_k` can be used to specify the number of weights to be printed for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\tcontains_positive_term\t1.00\n",
      "\n",
      "neg\tcontains_negative_term\t-1.54\n"
     ]
    }
   ],
   "source": [
    "simple_classifier.print_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2 (5 pts)**\n",
    "\n",
    "The above output only prints two weights, one for each class. Why is this? Make sure to explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**  \n",
    "In simple_featurizer, there is only contains_positive_term and contains_negative_term these 2 feats, so we only have 1 feat for each class and thats why only prints two weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3 (5 pts)**\n",
    "\n",
    "What does the output of `print_weights` tell us about the features of `simple_featurizer`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**  \n",
    "It tells us that lable positive have a feat named contains_positive_term which weight 1.00 and lable negative have a feat contains_negative_term which weight -1.54. So simple_featurizer have 2 feats and the negative term have more weights than positive term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Bag of Words (20 pts)\n",
    "Now, let's build a new featurizer method that converts a text into a *binary* bag of words (i.e. there is a feature for each observed word type and its value is set to 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1 (5 pts)**\n",
    "\n",
    "Complete the implementation of `bag_of_words` so that it returns a *dictonary* (`dict`) of feature-value pairs corresponding to the text's binary bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    \"\"\"\n",
    "    This method returns a binary bag of words representation of a text\n",
    "    \n",
    "    The return dict `feats` must have the following form:\n",
    "    - keys correspond to the name of the feature\n",
    "    - values correspond to the feature's value\n",
    "    \n",
    "    \"\"\" \n",
    "    feats = {}\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Your code here\n",
    "    for token in tokens:\n",
    "        if token not in feats:\n",
    "            feats[token] = 1\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `bag_of_words` implemented, we can see how well these features work for our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizer: bag_of_words, # Features: 40970\n",
      "Training Accuracy: 100.0%, Validation Accuracy: 84.7%\n"
     ]
    }
   ],
   "source": [
    "bow_classifier = LogisticRegression(bag_of_words)\n",
    "bow_classifier.train_model(\"train.tsv\")\n",
    "evaluate_classifier(bow_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike our first classifier (`simple_classifier`), our bag of words classifier (`bow_classifier`) has a dramatically higher training accuracy than validation accuracy. Moreover, our classifier reaches perfect accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2 (5 pts)**\n",
    "\n",
    "Why is the training accuracy so high? Explain your reasoning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**  \n",
    "The bow_classifier is performing much better than simple_classifier since it is basically make vocabulary into a dictionary, and compare the testing data with existing training set. In this way, it transfer the language into a vector. In this way, our training accuracy will be very high since we trained with it and we have all these vectors in our model already. While testing, there might be some new words that is not in the vocab dictionary, which affect the validation accuracy to be lower than training one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LogisticRegression` class includes two ways to reduce overfitting.\n",
    "\n",
    "Firstly, the parameter `min_feature_df` specifies the minimum number of documents a feature must appear in during training to be considered a feature for the model. This is especially helpful for bag of words style features whose presence depends on the texts seen during training.\n",
    "\n",
    "Secondly, our model uses L2 regularization. We can specify the strength of the L2 regularization with the `regularization_inverse_strength` parameter. This parameter must be a positive value. Smaller values correspond to stronger regularization, while larger values correspond to weaker regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3 (5 pts)**\n",
    "\n",
    "Let's examine the effects of `regularization_inverse_strength`. As the value of `regularization_inverse_strength` decreases, how do the weights and accuracies of the bag of words classifier change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization_inverse_strength = 1.0\n",
      "Featurizer: bag_of_words, # Features: 40970\n",
      "Training Accuracy: 100.0%, Validation Accuracy: 84.7%\n",
      "pos\tgreat\t1.27\n",
      "pos\tentertaining\t1.08\n",
      "pos\t7/10\t1.02\n",
      "pos\trealistic\t0.97\n",
      "pos\tsuperb\t0.93\n",
      "\n",
      "neg\tworst\t-2.23\n",
      "neg\twaste\t-1.37\n",
      "neg\tawful\t-1.37\n",
      "neg\tboring\t-1.26\n",
      "neg\tbad\t-1.21\n",
      "\n",
      "regularization_inverse_strength = 0.1\n",
      "Featurizer: bag_of_words, # Features: 40970\n",
      "Training Accuracy: 99.2%, Validation Accuracy: 84.9%\n",
      "pos\tgreat\t0.82\n",
      "pos\tentertaining\t0.56\n",
      "pos\texcellent\t0.52\n",
      "pos\tlove\t0.51\n",
      "pos\tperfect\t0.50\n",
      "\n",
      "neg\tworst\t-1.24\n",
      "neg\tbad\t-0.79\n",
      "neg\tawful\t-0.74\n",
      "neg\twaste\t-0.74\n",
      "neg\tboring\t-0.70\n",
      "\n",
      "regularization_inverse_strength = 0.001\n",
      "Featurizer: bag_of_words, # Features: 40970\n",
      "Training Accuracy: 84.8%, Validation Accuracy: 80.5%\n",
      "pos\tgreat\t0.13\n",
      "pos\tlove\t0.08\n",
      "pos\tbest\t0.07\n",
      "pos\twell\t0.07\n",
      "pos\talso\t0.06\n",
      "\n",
      "neg\tbad\t-0.15\n",
      "neg\tworst\t-0.11\n",
      "neg\t?\t-0.10\n",
      "neg\tno\t-0.09\n",
      "neg\teven\t-0.08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inv_str in [1.0, 0.1, 1e-3]:\n",
    "    print(\"regularization_inverse_strength = {}\".format(inv_str))\n",
    "    model = LogisticRegression(bag_of_words, regularization_inverse_strength=inv_str)\n",
    "    model.train_model(\"train.tsv\")\n",
    "    evaluate_classifier(model)\n",
    "    model.print_weights()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**   \n",
    "For lowering the regularization_inverse_strength from 1.0 to 0.1, feats printed out for both pos and neg lable are having lower weight since it wants to prevent overfitting. Also, 7/10 appears on feat for regularization_inverse_strength=1.0 which it does not mean any pos or neg to me, it might just be a number frequently appear in the pos lable's training set. Also, the training accuracy and validation accuracy does not change a lot in this case.  \n",
    "For lowering the regularization_inverse_strength from 0.1 to 0.01, feats printed out for both pos and neg lable are having lower weight since it wants to prevent overfitting. However, since the weight for each feats are too low, it is having negative impact on our result. Both training accuracy and validation accuracy drops. This might because this too low weights, it make our model difficult to compare these vectors transformed by the dataset in words. For example, while regularization_inverse_strength=0.1, the biggest difference between weights of pos and neg is 0.82-(-1.24) = 2.06; while regularization_inverse_strength=0.01, the biggest difference between weights of pos and neg is 0.13-(-0.15) = 0.28 which is much smaller than before which result the accuracy drop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4 (5 pts)**\n",
    "\n",
    "Now, try improving the general performance (i.e. validation accuracy) of your bag of words classifier by changing both the `regularization_inverse_strength` and `min_feature_df` parameters in the cell below.\n",
    "\n",
    "*Note there is no single \"right\" answer for this question.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only modify the regularization_inverse_strength and min_feature_df parameters\n",
    "final_bow_classifier = LogisticRegression(bag_of_words,\n",
    "                                          regularization_inverse_strength=0.2,\n",
    "                                          min_feature_df = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizer: bag_of_words, # Features: 14651\n",
      "Training Accuracy: 99.6%, Validation Accuracy: 85.0%\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "#   DO NOT MODIFY!   #\n",
    "######################\n",
    "final_bow_classifier.train_model(\"train.tsv\")\n",
    "evaluate_classifier(final_bow_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Designing Features (50 pts)\n",
    "\n",
    "In this core part of the assignment, you will design *three* additional classes of features. For each class you will: (1) describe its features, (2) justify why you believe these features *should* perform better than a bag of words, (3) implement the corresponding featurizer, and (4) evaluate the performance of a logistic regression classifier that relies on these features. *These classes of features must be distinct*.\n",
    "\n",
    "The most creative features will receive extra credit for this assignment (up to 10 points). For evaluating extra credit, we will consider the uniqueness of the features (relative to other student submissions), the quality of their written descriptions and justification, and the performance of these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Feature 1 (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1.1**\n",
    "\n",
    "In a few sentences, describe this class of features and explain why you believe that these features will perform better than a bag of words for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**  \n",
    "My feat1 is basically an improvement of bag_of_words plus an stopwords remove. Since we have not consider words such as 'our', 'i', 'the' words like this which does not provide information about pos and neg. In this way, we remove them from our dictionary return so that this would increse the prediction performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1.2**\n",
    "\n",
    "Implement the featurizer for this class of features in the following cell. Then, train a classifier that relies on this featurizer in the cell after that. Tailor the values for `regularization_inverse_strength` and `min_feature_df` to suit this feature representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def feat1(text):\n",
    "    \"\"\"\n",
    "    This method returns a bag of features\n",
    "    \n",
    "    The return dict `feats` must have the following form:\n",
    "    - keys correspond to the name of the feature\n",
    "    - values correspond to the feature's value\n",
    "    \n",
    "    \"\"\" \n",
    "    feats = {}\n",
    "    \n",
    "    # Your code here\n",
    "    avoid = stopwords.words('english')\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    for token in tokens:\n",
    "        if token in avoid:\n",
    "            continue\n",
    "        if token not in feats:\n",
    "            feats[token] = 1\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only modify the regularization_inverse_strength and min_feature_df parameters\n",
    "feat1_classifier = LogisticRegression(feat1,\n",
    "                                      regularization_inverse_strength=0.1,\n",
    "                                      min_feature_df = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizer: feat1, # Features: 20065\n",
      "Training Accuracy: 98.8%, Validation Accuracy: 85.2%\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "#   DO NOT MODIFY!   #\n",
    "######################\n",
    "feat1_classifier.train_model(\"train.tsv\")\n",
    "evaluate_classifier(feat1_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see on the result printed out, both accuracy are better than the question2.4, althogh it is just improved by a little, it is still a progress I think."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature 2 (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2.1**\n",
    "\n",
    "In a few sentences, describe this class of features and explain why you believe that these features will perform better than a bag of words for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**  \n",
    "This feat2 is basically same as bag of words. But I avoid the word with too short and I give different word with different weight in dictionary feats. If the word appearing more than 1 time, it will have a weight up to 5.  \n",
    "I believe these feature will perform better than a bag of words since word that is too short are meaning less in most cases, such as 'I', 'a', 'an', 'of' or other words like these. Also, if we assign more weight to a word such as 'good' which always show up in the pos case, it will help us to have a better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3.2**\n",
    "\n",
    "Implement the featurizer for this class of features in the following cell. Then, train a classifier that relies on this featurizer in the cell after that. Tailor the values for `regularization_inverse_strength` and `min_feature_df` to suit this feature representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat2(text):\n",
    "    \"\"\"\n",
    "    This method returns a bag of features\n",
    "    \n",
    "    The return dict `feats` must have the following form:\n",
    "    - keys correspond to the name of the feature\n",
    "    - values correspond to the feature's value\n",
    "    \n",
    "    \"\"\" \n",
    "    feats = {}\n",
    "    \n",
    "    # Your code here\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    for token in tokens:\n",
    "        if len(token)>2:\n",
    "            if token not in feats:\n",
    "                feats[token] = 1\n",
    "            else:\n",
    "                feats[token] +=1\n",
    "    \n",
    "    for feat in feats:\n",
    "        if feats[feat]>5:\n",
    "            feats[feat]=5\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only modify the regularization_inverse_strength and min_feature_df parameters\n",
    "feat2_classifier = LogisticRegression(feat2,\n",
    "                                      regularization_inverse_strength=0.15,\n",
    "                                      min_feature_df = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizer: feat2, # Features: 14369\n",
      "Training Accuracy: 99.4%, Validation Accuracy: 85.9%\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "#   DO NOT MODIFY!   #\n",
    "######################\n",
    "feat2_classifier.train_model(\"train.tsv\")\n",
    "evaluate_classifier(feat2_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature 3 (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3.1**\n",
    "\n",
    "In a few sentences, describe this class of features and explain why you believe that these features will perform better than a bag of words for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**  \n",
    "This feat2 is basically same as bag of words. But I avoid the word with too short and I give different word with different weight in dictionary feats. In addition, words appearing too much in all training datasets, maybe meaningless for determine pos and neg, so I convert their weight into 1/weight so that this could reduce missleading while predict.\n",
    "I believe these feature will perform better than a bag of words since word that is too short are meaning less in most cases, such as 'I', 'a', 'an', 'of' or other words like these. Also, the 1/weight also helps for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3.2**\n",
    "\n",
    "Implement the featurizer for this class of features in the following cell. Then, train a classifier that relies on this featurizer in the cell after that. Tailor the values for `regularization_inverse_strength` and `min_feature_df` to suit this feature representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat3(text):\n",
    "    \"\"\"\n",
    "    This method returns a bag of features\n",
    "    \n",
    "    The return dict `feats` must have the following form:\n",
    "    - keys correspond to the name of the feature\n",
    "    - values correspond to the feature's value\n",
    "    \n",
    "    \"\"\" \n",
    "    feats = {}\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    for token in tokens:\n",
    "        if len(token)>2:\n",
    "            if token not in feats:\n",
    "                feats[token] = 1\n",
    "            else:\n",
    "                feats[token] +=1\n",
    "    \n",
    "    for feat in feats:\n",
    "        if feats[feat]>10:\n",
    "            feats[feat] = 1/feats[feat]\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only modify the regularization_inverse_strength and min_feature_df parameters\n",
    "feat3_classifier = LogisticRegression(feat3,\n",
    "                                      regularization_inverse_strength=0.2,\n",
    "                                      min_feature_df = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizer: feat3, # Features: 14369\n",
      "Training Accuracy: 99.7%, Validation Accuracy: 85.9%\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "#   DO NOT MODIFY!   #\n",
    "######################\n",
    "feat3_classifier.train_model(\"train.tsv\")\n",
    "evaluate_classifier(feat3_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Comparing Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4 (5 points)** Now, compile the results for the classifiers that you have built using each of these features as well as your bag of words features from Part II. Add these results to the table below. Then, comment on these results. Which of these features were the best and worst performers? Were these results surprising? Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature | # Features | Training Accuracy | Validation Accuracy |\n",
    "| :- | -: | -: | -: |\n",
    "| Bag of Words | A dictionary that contain all vocab|99.6% | 85.2%|\n",
    "| Feature 1 | A dictionary that contain all vocab without the word meaningless in the top feat of bag_of_word| 99.6%| 85.3%|\n",
    "| Feature 2 | A dictionary that contain all vocab in different weight without words that are too short|99.4% | 85.9%|\n",
    "| Feature 3 | A dictionary that contain all vocab in different weight without words that are too short and if one feat appears too much, lower its weight in dict feats| 99.5%| 85.9%|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Combining Features (10 pts)\n",
    "And finally, you'll creating an ultimate feature reprensentation in order to make predictions for our test set in `test.tsv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.1 (5 points)**\n",
    "\n",
    "Implement `final_features` so that it produces a feature representation that is a combination of any and all of the features that you have developed through the course of this assignment (i.e., `bag_of_words`, `feat1`, `feat2`, and `feat3`). *Your code must directly call these previously implemented featurizer methods.*\n",
    "\n",
    "Then, train a classifier that relies on `final_features` in the cell after that. Tailor the values for `regularization_inverse_strength` and `min_feature_df` to suit your class of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_features(text):\n",
    "    \"\"\"\n",
    "    This method returns a bag of features\n",
    "    \n",
    "    The return dict `feats` must have the following form:\n",
    "    - keys correspond to the name of the feature\n",
    "    - values correspond to the feature's value\n",
    "    \n",
    "    \"\"\" \n",
    "    feats = {}\n",
    "    \n",
    "    # Your code here\n",
    "    avoid = stopwords.words('english')\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    for token in tokens:\n",
    "        if len(token)>2:\n",
    "            if token in avoid:\n",
    "                continue\n",
    "            elif token not in feats:\n",
    "                feats[token] = 1\n",
    "            else:\n",
    "                feats[token] +=1\n",
    "    \n",
    "    for feat in feats:\n",
    "        if feats[feat]>10:\n",
    "            feats[feat] = 1/feats[feat]\n",
    "        if feats[feat]>5:\n",
    "            feats[feat]=5\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizer: final_features, # Features: 14267\n",
      "Training Accuracy: 99.1%, Validation Accuracy: 85.8%\n"
     ]
    }
   ],
   "source": [
    "# Only modify the regularization_inverse_strength and min_feature_df parameters\n",
    "final_classifier = LogisticRegression(final_features,\n",
    "                                      regularization_inverse_strength=0.1,\n",
    "                                      min_feature_df = 3)\n",
    "#final_classifier.train_model(\"train.tsv\")\n",
    "#evaluate_classifier(final_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.2 (5 points)**\n",
    "\n",
    "Finally run the following code cell to make predictions for our test set and save them to the file `final_predictions.tsv`. Upload this file as part of your code submission to Gradescope.\n",
    "\n",
    "The five systems with the highest performance will receive 5 points of extra credit for this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Feature index already exists\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "#   DO NOT MODIFY!   #\n",
    "######################\n",
    "final_classifier.train_model(\"train.tsv\")\n",
    "predictions = final_classifier.predict(\"test.tsv\")\n",
    "\n",
    "with open(\"final_predictions.tsv\", mode=\"w\", encoding=\"utf-8\") as writer:\n",
    "    with open(\"test.tsv\", encoding=\"utf-8\") as reader:\n",
    "        for i, line in enumerate(reader):\n",
    "            review_id = line.split('\\t')[0]\n",
    "            writer.write(\"{}\\t{}\\n\".format(review_id, predictions[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource: Analyzing Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will go over a few methods for analyzing classifiers. You may find these methods useful for identifying ways to improve the performance of your classifiers. **Nothing in this section will be graded.** Consider these as tools that you might use to support the design of your feature representations in Parts III and IV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrices\n",
    "\n",
    "We can examine the confusion matrix for a model's predictions. Are certain mistakes more likely than others? (Note that this tends to be more useful in multi-class classification rather than binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(filename):\n",
    "    labels = []\n",
    "    with open(filename, encoding=\"utf-8\") as reader:\n",
    "        for line in reader:\n",
    "            label = line.split('\\t')[1]\n",
    "            labels.append(label)\n",
    "    return labels\n",
    "\n",
    "def view_confusion_matrix(classifier, filename):\n",
    "    # Get true and predicted labels\n",
    "    true_labels = load_labels(filename)\n",
    "    predictions = classifier.predict(filename)\n",
    "    \n",
    "    sklearn.metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        true_labels, predictions, labels=[\"pos\", \"neg\"],\n",
    "        cmap=plt.cm.Purples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVkAAAESCAYAAABXbaZ0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAy80lEQVR4nO3dfXzN9f/H8cc5u2btAltb1hjDsGlDLnLVpAvmG0XJ9RAWVnO5RrlM+IbvMJQlDRPf9VVp9JsIESERM4Y1xdiF1bAZuzq/P5ZTx2znYGf7OOd173ZuN+fz/nzOeR309N77vD/vt0qj0WgQQghhFOrqLkAIIUyZhKwQQhiRhKwQQhiRhKwQQhiRhKwQQhiRZXUXUJVu3rxJYmIiLi4uWFhYVHc5Qpic4uJisrKy8PX1xdbW9r5fJycnh9zcXIPOtbe3x8nJ6b7fy9jMKmQTExMZOHBgdZchhMmLjY2ldevW93VtTk4O7Vp3QmNRYND5jo6ObN++XbFBa1Yh6+LiAkCt37tiUVSjmqsRhtpwLqy6SxAGSk9PZ/CQQdr/1+5Hbm4uGosCal94BosiuwrPLbbMJ5ud5ObmSsgqwe0hAouiGlgW1azmaoSh6tatW90liHtUGcNxlpoaWGrsKzxHpVE98PsYm1mFrBDi4aFSlT70naN0ErJCCGUykZSVkBVCKJIKAzK2Sip5MBKyQghFUqlVesdcVWrlx6yErBBCmWS4QAghjEelVqGWnqwQQhiJCv2DrsrPWAlZIYQyqVQqvT1VlQwXCCHE/TGRIVkJWSGEQplIykrICiEUSW0Baj0hqn4IFmuVkBVCKJOJ9GQfgn8HhBDm6HbG6nvcj+nTp/Pqq69qn1+/fp2JEyfSqlUrOnXqxJo1a3TO19deEenJCiEUSYVK7+wB1X3M4Tp48CD//e9/adGihfbYtGnTyMzMJDY2lvPnzxMREYGrqytBQUEGtVdEQlYIoUyG5Oc9Zmx+fj7vvvsuLVu2pKioCIC0tDS2b99OfHw83t7e+Pj4cO7cOWJiYggKCtLbro8MFwghFEmlVhn0uBeRkZG0atWK9u3ba48dO3YMJycnvL29tcdat27NyZMnKSws1Nuuj/RkhRDKZMACMbcHZdPT08s0OTg44ODgoH1+7Ngxtm7dSnx8POvWrdMez8jIwNXVVedaFxcXioqKuHLlit52d3f3CkuUkBVCKJJKpdI75np7zPZue/eNGzeO0NBQAAoKCpg6dSpTp04ts01Nfn4+NjY2Osesra211+lr10dCVgihTIasXfCX2NhY3NzcdI79sxe7fPly6tWrR48ePcpca2trWyYsbz+3tbXV266PhKwQQpEM6sn+1e7m5oaHh0e553399ddkZWUREBAAQGFhIcXFxQQEBDBjxgyysrJ0zs/MzMTKygpnZ2fc3NwqbNdHQlYIoUgqVeUtwrVu3TrtbILbz3/66SeWLFmCpaUl2dnZpKam4uXlBcCRI0fw9fXF2toaf3//Ctv1kdkFQghFqszZBXXr1qVevXrah6OjI9bW1tSrV4+6desSGBhIeHg4SUlJJCQksHr1aoYOHaq9tqJ2faQnK4RQJJVa/80GlXVT7fz585k+fTr9+/fH0dGRsLAwunfvbnB7RSRkhRDKpDLeqt2hoaHamQcATk5OLF26tNzz9bVXREJWCKFIBu1Wq6mSUh6IhKwQQpFUKgPXLlB40ErICiGUSY3+0QANUFwFtTwACVkhhCKp1GrUqoonQKk0aglZIYS4HypV6QyDCs8pqZpaHoSErBBCmUxkZwQJWSGEIplIxkrICiGUyZA7uu5nZ4SqJiErhFAmE+nKSsgKIRRJrQa1hZ4twauolgchISuEUCbpyQohhPGYSMZKyAohFMqQpQz17QGmABKyQghlMt4iXFVKQlYIoUhqtRq1RcVfbak1yv/qS0JWCKFIMiYrhBDGZCIpKyErhFAkg+74MnCPr+okISuEUCQT6chKyAohFMpEUlZC9iFg72TLmhPj+HrVT6ydsxuAOo89wphFLxDQtQElxSXs+m8iH0/dwc0bhdrr5n09kDYvNNJ5rT/Sc3nl8YVVV7xgf3wyn874jgtnsnF53IF+EzoQ9HorAHb9N5H3Bn5e5pp58QNp83yjMsfNiUG31cp6sqIyhPz7OWq52WufW1pZMHfLQAD+PeJLbGtaEbLgOZxd7ZkzIE57npfvo2xYsJf9XydrjxUVKHwZeRNzZGcKM/pspOeo1oR88Dw/7/yVxW98jWOdGnTs3ZTUxAy8fF2ZsPJfOtd5NnWppooVRHqyoir4d6lPp5eaceP6Le2xls80wPsJNwb7LOVSyh9AafBOWd2bR5ztuP5nPo842+Hi4cChb85y6uDF6irf7H3y7nd07tuMt5YFAdCyawPSUv7gyI4UOvZuyq8nMmjSui7N2j1ezZUqkAEZ+zDcjKD8mbxmzMrGkvEr/8XH7+zgZt7fwwDH9/7GuI4fawMWSnuoarUKS2sLALx8XQH4NTGzaosWWn9k5HL6cBo9/xoauG3Gxld5K6onAKmJmTT4689K6Lo9u0DfQ+kkZBVsyLtduHrlBl9/9JPO8Zt5BdreqZWNJX4dPBk2qys/bj3Dnxm5AHj5PUretVuELunOl5nhbMmOYNKqF7GtaV3ln8Nc/ZZU+g+chZUFk19Yyws15/Ca12K2rj4CwI3rt8j4LYekgxcZ1GQJz9nNZuxT0fKTx223hwv0PRSuykO2SZMmxMXF0b17d/z9/Rk1ahQZGRna9szMTCZOnEi7du1o3bo1U6ZM4erVq9r2DRs28Mwzz+Dr60vPnj359ttvq/ojVIkGfo/ycmg7Fr/xdYXnLds3gsjdw7F3suXjd3boXF/TwYbsS9d59+WNfDxtBx16NSV8dW8jVy5uy8m6AcD7g/+H71OPMy9+IB1e9GFxyNcc/L+zpJ7IQKOBjN+vErqkB7M+74eVjQXhPdZx5dK1aq6++plIxlZPT3bRokWEhYWxceNG8vLyGDNmDBqNhsLCQoKDg8nOzuaTTz4hOjqas2fPMnnyZACSkpKYO3cub7/9NgkJCbz44ouMHz+e7Ozs6vgYRqNWq5j40YtsXvoj509W/OP+yokJvB20ntSTmfznu2G4N3AG4PPIA4zvuoboqTs4se83tnx4mP+M+ZrOfZpRv7n8eFoVigpLv2Ts9FJThk4PJCCwAaFLetCqWwM2zPuees1cmPvVAD74v8G0faER7YOaMO/rgVjZWvJ55IFqrr76qdRqVBZ6Hmrl/zBeLV98DRs2jOeffx6AefPm8eyzz5KYmEhWVhYXLlxg/fr11KpVC4CFCxfSo0cPTp8+TVpaGgDu7u7UrVuXkSNH0qxZM+zs7KrjYxjNS6Ftcahdgw0L9uoskKFWq1BbqCkp/nveyi/fnwcgcf/vbEgZT4/hLVn9zk4uJF/hQvIVndc98m0KUDpeqy+8xYOzsy8dmmn9nLfO8YCuDfhswV7snexo16PxHdfY0Lz946TKWLqpTC6onpBt1ervLwI8PT1xcnIiJSWFrKwsPDw8tAEL0LBhQxwdHUlJSaFr1640b96cPn364O3tTWBgIH379qVGjRrV8TGM5ql/+fBYA2fi/5yqc3zwO10Y/E4XXg9YgaePC3s+P6lty88t4HLqn9R2fwSALn2bk335Ook//K49x9q29I/7Zm5BFXwK8ViD0r/HhbeKdI4XFxajUqk4d+wyZ49epvuwljrtBTeLtAFtzuS22gd5U0vdty0uLsbCwgIbG5u7nl9cXExxcTF2dnZs2rSJI0eOsHv3bnbu3ElsbCxr167Fz8+vKkqvEv8Z8zU1HtH9vZgfP4i9X55i68dH8O3gSWhkD04euMCVtNKxu1pu9nj61GH3fxMBeDHkSaxtLQnt+LH2NTq+1JSCm0Wc/ulS1X0YM1a/uQu13e35/vMkOvZqqj1+OOEczdp5cO6XdBaO2kKzdo9T7695sX9m5nJy/+8EzwisrrIVQ6VSodLTVdXXrgTVErJJSUn4+/sDkJqayvXr1/Hx8SEjI4OLFy+SnZ1N7dq1ATh79iy5ubl4eXlx9OhR9u3bR2hoKK1bt2bixIkEBQWxZ88ekwrZi2fKjjEXF5WQfek6Z45cIu1sNv0mdmD2/15j3ZzdWNlYMvjdLuRk5rH149Jvrjd9sI958YOYFN2L7z47gbe/G0NnBPK/pT9qZyAI41Kr1QydHsjiN76mlrs9Tz7vza5NiST9eJHFO4Px9ncjdt73zHx1E8NmdUWlgrVz9uBYpwZBI1vpfwNTJ4t2378VK1bg6elJ7dq1mTVrFu3bt6dRo0Y0bNgQb29vJk2axJQpU7h16xazZs0iICAAX19fkpOT+fDDD6lVqxZdunThzJkzpKWl0bx58+r4GNUm79otJj4bQ8i/n2PK6t6oLdX8tD2FlZMTyLtWetPCoYRzvPvyZwx+pwuBr/py9coNNszfy4b5e6u5evMS9HorLCzVbFy4jy9XHOLxxnWY80V//DrWA2BhwlA+ens7kWPjKbxVRKtuDXlj4fPY1pDhAtVf30HoO0fpVBqNRlOVb9ikSRNCQkL45ptvyMrKomvXrkyfPh1HR0cALl++zJw5czhw4ABWVlZ069aN8PBwbXt8fDwrVqzgwoULuLi4MGTIEIKDgw1674sXL/LMM8/g8mtPLItqGusjikq2s3BmdZcgDJSWlsazz3Vj586deHh43Ndr3P7/tLPfeGrYOFd47o1bf/L9if880PsZW7V98TV+/Pi7trm7u7NixYpyr+3Zsyc9e/Y0VmlCCIWQ2QVCCGFE8sWXEEIYk0oF+sZcJWTLSk5O1n+SEMLsyXCBEEIY0e1bZ/Wdo3QSskIIRZIxWSGEMCKVuvSh7xylk5AVQiiSCv09VeX3YyVkhRBKZSLffEnICiGUyYDhgodhbxcJWSGEIsnsAiGEMCKZXSCEEEZU+sWX/nOUTkJWCKFMagNuq30IljpU/oCGEMIs3R4u0Pe4FxcuXGD06NEEBATQsWNHPvjgA4qKSrcHun79OhMnTqRVq1Z06tSJNWvW6Fyrr7080pMVQiiSykKFykLPmKye9n/SaDSEhITQsGFD/ve//3HlyhUmT56MnZ0d48aNY9q0aWRmZhIbG8v58+eJiIjA1dWVoKAgAL3t5Sk3ZFNTUw0uHsDLy+uezhdCiIqUTpPV98WX4a+XlZVFo0aNmDFjBs7OzjRo0IAXXniBQ4cOkZaWxvbt24mPj8fb2xsfHx/OnTtHTEwMQUFBetsrUm7Idu/e3aCuuEajQaVScerUKcM/rRBC6KFSGbBb7T2krKurK5GRkdrnp0+fZufOnfTt25djx47h5OSEt/ff27e3bt2alStXUlhYqLfdysqq3PctN2TXrl1rcPFCCFHp7mEjxfT09DJNDg4OODg43PWyF198keTkZHx9fRkyZAgbN27E1dVV5xwXFxeKioq4cuUKGRkZFba7u7uXW2K5IdumTZtyL7p16xbW1tYPxRw1IcRDypAvtv5qHzhwYJmmcePGERoaetfL5s+fT05ODrNmzWLChAn4+flhY2Ojc461delmlgUFBeTn51fYXhGDv/g6deoUy5cv5+DBg+Tl5REXF8dnn32Gp6cno0aNMvRlhBDCICq1AcMFf7XHxsbi5uam01ZeLxagWbNmALz33nsMGjSIJ598skxY3n5ua2uLra1the0VMWgK16FDh+jXrx85OTkMHz6c2xvcenh4EBkZybp16wx5GSGEMNjtkNX3AHBzc8PDw0PncWfIXrlyhYSEBJ1jjRo1Akp/Os/KytJpy8zMxMrKCmdnZ9zc3Cpsr4hBIbtgwQKee+451q9fz8iRI7UhGxISQkhICBs2bDDkZYQQwmAqDJgnew/3fF28eJE333yTCxcuaI+dPHkSS0tLevXqRXZ2ts6sqiNHjuDr64u1tTX+/v4VtlfEoJA9e/YsvXv3Lv3gd4yRtG3blkuXLhnyMkIIYbDbKx3qexiqRYsWPPHEE0RERHDmzBkOHDjAjBkzGDJkCHXr1iUwMJDw8HCSkpJISEhg9erVDB06FEBve0UMClkXF5dyp2glJydTp04dwz+pEEIYoLJDVq1WExUVRe3atRk4cCATJkzgueeeY8KECUDpl2Fubm7079+fuXPnEhYWRvfu3bXX62svj0FffPXr14+oqChUKhUdO3ZEpVJx7do1tm3bRlRUFMHBwYZ/UiGEMIAxVuFydXVlyZIld21zcnJi6dKl5V6rr708BoXsqFGjyM3NZenSpSxatAiNRsOwYcOwtLTktdde44033rjnNxZCiIqo1CrUBs4uUDKDp3BNmDCBESNG8Msvv5CTk4ODgwMtWrSgVq1axqxPCGGmTGT3mXtbIObWrVvcvHmTwsJC7QwDIYQwhspeu6C6GBSyN2/eZPr06WzdupXi4uK/L7a0ZNCgQYSHh8vdX0KISmVWi3b/+9//5rvvvmP69Ok8/fTTODs788cff5CQkEBkZCQ2NjaMHz/e2LUKIcyICv3zYO9lnmx1MShkt27dypQpU3j11Ve1x9zc3Bg6dChqtZqVK1dKyAohKpVZjcmqVKpy58J6eXnpXSBBCCHuldqA2QX62pXAoJsRBg8eTGRkJBkZGTrHc3Nz+eijjxgwYIBRihNCmDFDbkRQfsaW35N97bXXdJ6npKTQrVs3mjdvTu3atbl27RqJiYmUlJRUuNqNEELcFxMZLyg3ZO/cTubO5w4ODnh4eBinKiGE2TORjC0/ZOfNm1eVdQghhA5j3FZbHQy+GSE3N5ekpCSdGxFKSkrIz8/n2LFjhIeHG61IIYT5Mat5st999x2TJk0iPz9fe+z2BooAnp6eErJCiEplVrMLlixZQtOmTdm8eTN9+/alZ8+ebN26lYiICKytrZk6daqx6xRCmBmVgQ+lM6gnm5qaqg3a9u3bs3LlSho2bEjDhg25du0aK1eupEuXLsauVQhhRkxlTNagnqyVlRV2dnZA6SyD1NRU7Q0Ibdq04ddffzVehUII82Qi82QNCtknnniCL7/8EoAGDRqgUqk4cOAAAOfPn0etNuhlhBDCYHr39zJky3AFMGi4YNy4cQwbNow//viDVatW8corrzBp0iRatmzJwYMH6dGjh7HrFEKYGbVa/xdbD0P/zqCQbdmyJdu2bePcuXMATJs2jVq1avHLL78wbNgwRo0aZdQihRDmx6ymcEHpbo1169YFwMLCgnHjxhmtKCGEMPk7vhYvXnxPL3R7x0chhKgMpjK7oNyQjY+PN/hFVCqVhKwQolKZfE/2u+++q8o6qtTa06HaoQ+hfC84vFfdJQgDFVnkwmOV9GKGzB54CFL2njZSFEKIqqJSq/Ru+W1SW4ILIURVMvkxWSGEqE5mN4VLCCGqkqn0ZO/rfolbt25p15QVQghj0LdugSGzD5TA4J7sqVOnWL58OQcPHiQvL4+4uDg+++wzPD095Y4vIUSlM6ue7KFDh+jXrx85OTkMHz5c24v18PAgMjKSdevWGbVIIYT5Uf21aHdFj4dhdoFBIbtgwQKee+451q9fz8iRI7UhGxISQkhICBs2bDBqkUII81M6HKBvFa7qrlI/g0L27Nmz9O7dGyjbPW/bti2XLl2q9MKEEOZNpVJp58qW+3gIUtagkHVxceHUqVN3bUtOTqZOnTqVWpQQQpjVF1/9+vUjKioKlUpFx44dUalUXLt2jW3bthEVFUVwcLCRyxRCmBtT+eLLoJAdNWoUubm5LF26lEWLFqHRaBg+fDgWFha89tprvPHGG8auUwhhZkxlt1qDp3BNmDCBESNGcOzYMa5evYqDgwMtWrSgVq1axqxPCGGmzKone5ujo6PsSiuEqBqGjLkqP2MNC9muXbvq/Rdj586dlVKQEEIAJrOgrEEh26NHjzIhe+PGDX7++WcuXbrEm2++aZTihBDmS4UBwwUPQVfWoJCdNGlSuW3h4eGcPXu20goSQggwmY7s/S0Q808vvfQSW7durYxahBBCS22hMuihdA+81OHx48dRPwybnwshHiq3b6vVd47SGRSyEydOLHOsuLiY9PR0jh8/zsCBAyu9MCGEeTOrKVyZmZlljqlUKpycnJg2bRr9+vWr9MKEEObNVMZkDQrZN998Ez8/P2xtbY1djxBC/MWQBWCUn7IGDaaGhISwY8cOY9cihBBa+pc5fDhW4TKoJ1unTh2KioqMXYsQQmiZ1doFffv2Zc6cOezfv5/69etTu3ZtnXaVSsWrr75qlAKFEObJGF98paen8/7773Pw4EEsLS3p0qULb7/9Ng4ODly/fp2ZM2eye/duatSowfDhwxk2bJj2Wn3t5TEoZBctWgTAli1b7touISuEqGyV/cVXSUkJY8eOxcnJiZiYGAoKCpg5cyYREREsX76cadOmkZmZSWxsLOfPnyciIgJXV1eCgoIA9LaXp9yQPXz4MM2aNaNmzZqcPn3a8E8ihBCVQGXAHl73ssdXcnIyiYmJ7Nu3DxcXF6A0OAcOHEhaWhrbt28nPj4eb29vfHx8OHfuHDExMQQFBeltr0i5X3wNGTKElJQUgz+AEEJUJhUG7IxwD6/n7u5OdHS0NmCh9KdwjUbDTz/9hJOTE97e3tq21q1bc/LkSQoLCzl27FiF7RUptyd7e7NEIYSoFobMHvirPT09vUyTg4MDDg4O2udOTk507txZ55xPP/0ULy8vsrOzcXV11WlzcXGhqKiIK1eukJGRUWG7u7t7uSU+8G21QghhDPdyW+3d7jodN24coaGh5V67atUqvv32W1atWsXx48exsbHRabe2tgagoKCA/Pz8CtsrUmHIxsXF8f3331f4AlD6GzF27Fi95wkhhKHu5Yuv2NhY3NzcdNr+2Yu90/Lly1m6dCnTp0+nU6dOnDlzpkxY3n5ua2uLra1the0VqTBkt2/fjpWVVYUvcJuErBCiMt3LFC43Nzc8PDwMet3333+ftWvXMnPmTPr376+9PisrS+e8zMxMrKyscHZ21ttekQpDNjo6mhYtWhhUuBBCVCZjrF0QFRXF+vXrmT9/Pr1799Ye9/f3Jzs7m9TUVLy8vAA4cuQIvr6+WFtb622viKxRKIRQpMq+rTY5OZnly5czYsQIOnToQFZWlvbh5uZGYGAg4eHhJCUlkZCQwOrVqxk6dCgAdevWrbC9IvLFlxBCkdQqA26rvYeQ3b59OyUlJaxatYpVq1bptG3bto358+czffp0+vfvj6OjI2FhYXTv3l17jr728pQbsi+99JLesQYhhDCaSt6tNjQ0tMLZBgBLly4tt83JyanC9vKUG7Lz5s275xcTQojKYlbryQohRFVT/fWfvnOUTkJWCKFI0pMVQggjMqs9voQQoqqZ1aLdQghR1WS4QAghjMo0NlKUkBVCKJMK/Rmq/IyVkBVCKNO9LHWoZBKyQghFki++hBDCiExktEBC9mHxy/fnmfL8unLbB03rzPq5d19gvUXnenyQMMRYpYm7sHeyZdXhELZ98jPr55X+uTi51GT4rK607OqFbQ1rzh67TPS0Hfx6IkN7XdM2dXn9vW409HuUPzLz+GrlIb768HB1fYxqJfNkRZXy9ncncrfuHu/X/8xnTv/PCeznywvBAbR+tqFOe+L+C3w8dQfPD/WvwkoFwMj3ulHrUXvtc7VaxcxNr+JQy46P39lJ7tWbvDy2LR98M4TRbT/iSto13Bs4897mAfz07TnWz/ser+aujJzbjeKiEuI/PlKNn6Z6yBQuUaVqOtjQtK3uyu/zhmymTl0Hxix+AdsaVrh4/L3dxs0bhSwY9iWB/XzpNkAWXq9KLTrVo8OLPty4fkt7rHn7x/FpXZexHaNJOV7acz2x7zdiToby/OAniJ2/lx7BAVz/I58Fr39JSbGGo7tSqdfUhR7DWpppyJpGT1YW7X5Indx/gd1xJxm94Flsa5TdImjzsh/Jycpj5Lxu1VCd+bKyseCtpUGsmfUdN2/8vVV0YUExW1cf0QYswK38Iq5cvMajnk4A/G/ZQWb020RJ8d87RRcVFGNlY1Fl9SuNvi3BHwYSsg+pT2fuwq+jJ+2CGpdpy7t6k7jFB3j5zbbUdn+kGqozX4Pe7szVK3lsXf2zzvHTh9NYNv4bnWOujzvi2dSFC2euAJCTlcdvp0r3karhYEPgq6U/hfxfzLEqqV1pbs8u0PdQuioN2SZNmvDll1/y0ksv4efnR69evTh+/Li2PSMjgzfffJOAgAA6derEzJkzycvL07afPn2a/v3706JFC3r16sWaNWvo2rVrVX4ERfj1RAbH9/5G3/Ht79r+7frjFBUU82LIk1VcmXnzau5KrzfasOTNbXrPVVuoeGtpD27dKGT7ul902mo42LD54mTCP+7N+aRM/m/tUWOVrGj6erEPS2+2ynuykZGRhIWF8dVXX1GzZk1mzpwJgEajYdy4cVhZWREXF0dUVBSnT59m6tSpAFy/fp3hw4dTv359vvjiC4YNG3Zfq5Sbgm8+OYq7lzNtuze6e/uan+nySnOcXGpWcWXmS61WERYVxJcrDml7o+Wea6FiSnRv/Lt4sXD0Fq5m39BpLykuIbzneuaP+AInF3sWxA9GbfEQpEklq+w9vqpLlX/xNXjwYLp06QLAiBEjGDNmDMXFxRw6dIjU1FQ2bNig3YZ83rx5vPDCC6Snp7Nnzx7UajWzZs3C2tqahg0bcu7cObZt099rMDUH4pMJ7Od7179gl1P/5PzJLF5/X8Ziq1KvN57EoVYNNi3+QScQVWoVaguVdpzVxs6Saev60jLQiw9Gf8XB/ztb5rVu5hXyy/fnAcj8/SqLvw0m4Gkvjuz8tUo+i6hcVR6y9evX1/7a3r50iktxcTEpKSnk5ubSpk2bMtekpqaSnJxMs2bNdLbf9ff3N7uQ/e1UFlkXr9Ghl89d23/ankINBxsCAr2quDLz1r5HE9y9nPni0hSd4wPDOzEwvBMvOLyHnb017385gIYt3Hh/6Gb2xyfrnBsQ6EVRQTEnfvhde+zXxNIvyv45HcxcmMrsgioP2du91H/SaDQUFRXh6elJdHR0mXYXFxd27dpFSUlJVZSoaGd+voSVtQXe/m7ltjcKcMfSyny/ka4OS9/ait0jNjrH3tvcnx+2nOabT0vHVN9Z1wcv30eZ0W8TR3ellnmNF4b4U7+ZK2+0X0VJSWnP179zfQB+O13xEIQpknmylaxhw4akp6fzyCOPUKtWLQBSUlJYuHAhs2bNolGjRnzzzTcUFhZqg/rEiRPVWXK1+C0pCzcvp3JD9LekLLwD3Ku4KnHx3B9ljhUXlfBHei5nj17m6b7NafVMQ7766DD5uQX4PFlXe97V7Btc/vVPPl/6I4u/DWbSql58u/4XHmvozNB3n+aHr09z5ufLVflxFEFCtpJ16NCBhg0bMmHCBCZPnoxGo2H69OlYW1vj6upKz549iYyMZPbs2QQHB5OcnMy6detwcnKq7tKr1NUrN7B3tNXTblNuu6get6fa9Rr9JL1G68762P2/k8wf9gVnj15maq9Yhs0MZMZnr5B37RYJa4+x9r091VGyAujfSPFhWL1AMSGrVqtZsWIFc+fOZdCgQVhZWdGxY0ft7AI7Ozs++ugjZs6cSa9evWjUqBF9+/Zlzx7z+gs48aMXK2yPOVXxvvKi6gxoFKn99fxhXzB/2Bd6rznxw+9MeDbGiFU9ZJSfoXpVacgmJ+sO9Ldt21bnmLu7O1FRUXe99sKFC+Tn5/P5559rj61atYpHH33UOMUKIaqVqQwXPDR3fOXl5REcHMyWLVtIS0tj3759rF27lh49elR3aUIIIyhd6lDff8qnmOECfXx8fJg9ezbLly/n0qVLuLi4EBwcTL9+/aq7NCGEEZhKT/ahCVmAPn360KdPn+ouQwhRBWSerBBCGJOJbI0gISuEUCQTyVgJWSGEMpnKcMFDM7tACCEeRtKTFUIoksqARblVsmi3EEKYN+nJCiEUSYUB82SrpJIHIyErhFAkQ+7pehju+ZKQFUIok4nM4ZKQFUIoklpV+tB3jtJJyAohlMlEFi+QkBVCKJKJjBZIyAohlMlEOrISskIIhTKRlJWQFUIokgwXCCGEMRmwQIz0ZIUQ4j6ZyGiBrF0ghBDGJD1ZIYQimcraBdKTFUIomErP4/7cunWLoKAgvv/+e+2x69evM3HiRFq1akWnTp1Ys2aNzjX62ssjPVkhhCIZa0w2Pz+f8ePHc+7cOZ3j06ZNIzMzk9jYWM6fP09ERASurq4EBQUZ1F4eCVkhhDIZYQ5XYmIi4eHhWFlZ6RxPS0tj+/btxMfH4+3tjY+PD+fOnSMmJoagoCC97RWR4QIhhCKpDPzvXhw4cIAuXbqwceNGnePHjh3DyckJb29v7bHWrVtz8uRJCgsL9bZXRHqyQoiHXnp6epljDg4OODg46BwbOXLkXa/PyMjA1dVV55iLiwtFRUVcuXJFb7u7u3u5tUnICiEU6V7GZAcOHFimbdy4cYSGhhr0Xvn5+djY2Ogcs7a2BqCgoEBve0UkZIUQD73Y2Fjc3Nx0jt3Zi62Ira1tmbC8/dzW1lZve0UkZIUQiqQy4Lba2+1ubm54eHjc93u5ubmRlZWlcywzMxMrKyucnZ31tldEvvgSQiiTvimyDzZVVoe/vz/Z2dmkpqZqjx05cgRfX1+sra31tldEQlYIoUhVmLHUrVuXwMBAwsPDSUpKIiEhgdWrVzN06FCD2isiwwVCCGWq4hVi5s+fz/Tp0+nfvz+Ojo6EhYXRvXt3g9vLIyErhDBLycnJOs+dnJxYunRpuefray+PhKwQQrEehgVg9JGQFUIo0r3MLlAyCVkhhHIpP0P1kpAVQiiS7PElhBDGZCIpKyErhFAo00hZCVkhhCKZRsRKyAohFMpUdquVkBVCKJOJpKysXSCEEEYkPVkhhCKZSEdWerJCCGFMZtWTLS4uBiAjo+x+QEK5iixyq7sEYaAiixvA3/+vPYiMjAy9t81mZGQ88PsYm1mF7O2VzYOH6V8DUijIY9VdgLhXWVlZ1KtX776utbe3x9HRkcFDBhl0vqOjI/b29vf1XlVBpdFoNNVdRFW5efMmiYmJuLi4YGFhUd3lCGFyiouLycrKwtfXV+/eVxXJyckhN9ewn2Ds7e1xcnK67/cyNrMKWSGEqGryxZcQQhiRhKwQQhiRhKwQQhiRhKwQQhiRhKwQQhiRhKwQQhiRhKwQQhiRhKwQQhiRhKwQQhiRhKwQQhiRhKyCNWnShLi4OLp3746/vz+jRo3SWXUoMzOTiRMn0q5dO1q3bs2UKVO4evWqtn3Dhg0888wz+Pr60rNnT7799tvq+BhmpUmTJnz55Ze89NJL+Pn50atXL44fP65tz8jI4M033yQgIIBOnToxc+ZM8vLytO2nT5+mf//+tGjRgl69erFmzRq6du1aHR9FVBIJWYVbtGgRYWFhbNy4kby8PMaMGYNGo6GwsJDg4GCys7P55JNPiI6O5uzZs0yePBmApKQk5s6dy9tvv01CQgIvvvgi48ePJzs7u5o/kemLjIwkLCyMr776ipo1azJz5kwANBoN48aNw8rKiri4OKKiojh9+jRTp04F4Pr16wwfPpz69evzxRdfMGzYMJYuXVqNn0RUCo1QrMaNG2s+/PBD7fPffvtN07hxY83x48c1O3fu1Pj6+mqys7O17efOndM0btxYc+rUKc327ds1zZo105w4cUKj0Wg0JSUlmr1792ry8vKq/HOYk8aNG2s+/vhj7fMdO3ZoGjdurCkqKtLs379f06pVK01BQYG2/ddff9U0btxYc/nyZc3GjRs1HTp00Ny6dUvb/sEHH2gCAwOr9DOIymVW68k+jFq1aqX9taenJ05OTqSkpJCVlYWHhwe1atXStjds2BBHR0dSUlLo2rUrzZs3p0+fPnh7exMYGEjfvn2pUaNGdXwMs1K/fn3tr2+vc1pcXExKSgq5ubm0adOmzDWpqakkJyfTrFkzrK2ttcf9/f3Ztm2b0WsWxiMhq3CWlrp/RMXFxVhYWGBjY3PX84uLiykuLsbOzo5NmzZx5MgRdu/ezc6dO4mNjWXt2rX4+flVRelmy8rKqswxjUZDUVERnp6eREdHl2l3cXFh165dlJSUVEWJogrJmKzCJSUlaX+dmprK9evX8fHxoUGDBly8eFFnjPXs2bPk5ubi5eXF0aNHiYqKonXr1kyaNIlt27bh7u7Onj17quNjCEp/0khPT+eRRx6hXr161KtXj6KiIubPn09ubi6NGjUiOTmZwsJC7TUnTpyoxopFZZCQVbgVK1awb98+Tp06RUREBO3bt6dRo0Y89dRTeHt7M2nSJE6dOsWxY8eYMmUKAQEB+Pr6Ymdnx4cffkhsbCwXL15k165dpKWl0bx58+r+SGarQ4cONGzYkAkTJnDy5EkSExOZPHkyf/75J66urvTs2ZOSkhJmz55NSkoK27ZtY926ddVdtnhAErIK16dPH2bPns2AAQOoW7cuS5YsAUCtVrNixQrs7OwYMGAAo0aNomnTpnz00UeoVCp8fHxYsGABsbGxdO/enffee4/x48cTGBhYzZ/IfN3+M7O3t2fQoEHamQRRUVEA2NnZ8dFHH3Hq1Cl69epFdHQ0ffv2vevwg3h4yPYzCtakSROio6Pp3LlzdZciqsCFCxdIT0/nySef1B5btWoV+/btY+3atdVYmXgQ0pMVQiHy8vIIDg5my5YtpKWlacO1R48e1V2aeAAyu0AIhfDx8WH27NksX76cS5cu4eLiQnBwMP369avu0sQDkOECIYQwIhkuEEIII5KQFUIII5KQFUIII5KQNUGDBw+mSZMm2oePjw8tW7bktdde4/vvvzfKey5btowOHTponzdp0oTPPvvMoGsvX75McHAwt27deqAaNm/eTJMmTcp9nTtr1OfgwYM0adKElJSUB6qra9euLFy48IFeQzy8ZHaBiXrqqad46623gNL75nNzc1m3bh0hISHExcUZ/c6vTZs28fjjjxt07oEDBzhw4IBR6xGiukjImignJyf8/f11jj355JN07tyZTZs2MXv2bKO+/53vLYS5kuECM2Jra0v9+vW5dOkSAG+//TZvvfUWoaGhPPHEE0yfPh0oXb0/LCyMVq1a0apVK+399f8UFxdHt27deOKJJ5g4cSI3b97Uab9zuCAxMZHg4GDtjgALFiygoKCAzZs3ExERAUCLFi3YvHkzULqA9bvvvkvbtm3x9/cnJCSEtLQ0nffYuXMn//rXv2jRogUjRowoU6M+165dY9asWXTu3BlfX186duzI+++/r7NAC8DPP/+sfZ+BAwfqLNoDcPToUe1uBp06dWLZsmWympbQkpA1I0VFRaSlpVG3bl3tsR07dmBvb8+KFSt4+eWXuXHjBkOGDOHMmTPMnTuXOXPmcPToUUaPHk1xcTEA27Zt49133+XZZ59l2bJlAHz66aflvu/FixcZPHgwAIsXLyYsLIy4uDgWLVrE008/zRtvvAHA+vXrefrppykpKWH06NHs2bOHiIgIFi5cSFZWFkOGDNFu1fLzzz8TGhqKr68vy5cvx8vLi//85z/39PsxYcIE9u/fT0REBNHR0bz88svExMTwxRdf6Jw3d+5cevfuzZIlSygpKdHuSAGl28UMHToUJycnli1bxsiRI1m9ejUffPDBPdUiTJcMF5io2+uXApSUlJCens6HH35IdnY2ffv21Tl3xowZ2NraAhAbG0taWhoJCQnaMG7WrBndu3dn165ddOvWjejoaJ577jnCw8MB6Ny5M2fOnOGPP/64ay1r166lZs2arFq1Srsg9Y0bN9i2bRu1atXC09MTKO3J2tjYsGfPHo4cOcKmTZu0ww5t2rTh6aefJi4ujuDgYD755BOaNm3KvHnzAOjUqZN2tTFD3Lx5k+LiYmbNmkW7du0AaN++Pbt37+bIkSO8+uqr2nPHjBnDiBEjAAgICCAwMJDPP/+c0aNHs3LlSh5//HGioqKwsLAAShd6mTVrFq+//jq1a9c2qB5huqQna6K++eYbmjdvTvPmzfHz8+PZZ59l165dzJ49W2fRbjc3N23AAhw+fJhGjRrx6KOPUlRURFFRER4eHnh6evLjjz+Sn5/PqVOnyixa061bt3JrOXr0KO3bt9dZ8X/w4MHlzj44dOgQTk5O+Pr6amuoUaMG/v7+HDx4ECjtyd5LDXeytbVlzZo1tG3blt9//509e/awatUqsrOzywwX/PN1b491//zzz0Dp71eHDh20/6gVFRXRqVMnCgsLtecI8yY9WRPVsWNHwsLCgNIl9hwcHPDw8EClUumcd2dPKycnh6SkpLvOPvDx8eHatWtoNBqcnZ112urUqVNuLVevXtXZJkefnJwccnJy7lrD7X8grl27dk813M2OHTuYO3culy5dok6dOvj7+2NjY8Odd5rf+Xvk7OxMenq6ttaYmBhiYmLKvH5mZuY91SNMk4SsiXJwcLivbWYcHBwICAhg2rRpZdocHR1xdHREpVKVGRrIyckp9zXt7e3LfCn1559/cvr0aZ09zP5Zwz/Xzv0nOzs7bS137rxbUQ13On/+PGFhYQwYMICRI0fi4uICwCuvvFLm3GvXruHo6Kh9np2drf1H45FHHqFnz5707t27zHWPPfaYwfUI0yXDBUJHQEAAv/32G15eXvj5+eHn50ejRo1YtmwZJ06cwNbWFj8/P7Zv365z3d69e8t9TX9/fw4cOEBBQYH22NatWxkzZgxQ2tO+s4bMzExq166trcHX15c1a9bwww8/AKXT0Xbu3KnT66yohjslJSVRWFjI6NGjtQF75coVzpw5U2ZmwP79+7W/vnLlCkePHqV169baWs+fP6+t08/PD0tLSyIjI2X7dQFIyIo79OnTBxsbG15//XW+/fZb9uzZw+jRozl8+DBNmzYFYNy4cezdu5dZs2axd+9e3nnnnTLTmv5p6NCh5ObmMmbMGPbs2UNcXBxLly5l0KBBWFtb4+DgAJSOI2dmZhIYGEijRo14/fXXiY+PZ//+/YwfP56EhASaNWsGQEhIiLY3unfvXhYtWsTOnTsN/pw+Pj5YWFgwf/58fvzxR7Zs2cKQIUO4desW+fn5OucuW7aMzz//nO+++45Ro0ZRu3Zt7ZeHISEh/Pjjj0RERLB3717i4+MZN24cGRkZeHl53dPvvTBNErJCh4ODA+vWrcPFxYW3336bCRMmABATE0ODBg0A6NKlC4sXL+bHH39k7NixZGRkMHr06HJfs169esTExJCfn09oaCjLly9nyJAh2jvS2rdvT7t27XjnnXf46quvsLKyYvXq1TzxxBPMmTOHMWPGcOnSJVauXKndNcDHx4fo6GguXLjA2LFjOXz4MJMnTzb4czZo0ID333+fY8eOMWrUKJYtW8YzzzxDSEgIJ06c0E5XA5g+fTrR0dG89dZbODs7ExMTo93q29/fn9WrV5OamsrYsWOZO3cuLVu25JNPPpFtYwQg68kKIYRRSU9WCCGMSEJWCCGMSEJWCCGMSEJWCCGMSEJWCCGMSEJWCCGMSEJWCCGMSEJWCCGMSEJWCCGM6P8BAFW2lRKrSWUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view_confusion_matrix(final_classifier, \"dev.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most defining features\n",
    "\n",
    "Next, we can examine the most defining features for each label class by looking at the weights of our model, specifically the ones with the highest magnitudes. Are your proposed features helping in the way that you expect? Do successful features suggest others? What features might provide a complementary view of the reviews and their sentiments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\tentertaining\t0.59\n",
      "pos\tgreat\t0.59\n",
      "pos\tsuperb\t0.54\n",
      "pos\texcellent\t0.54\n",
      "pos\tenjoy\t0.46\n",
      "pos\trealistic\t0.45\n",
      "pos\tdefinitely\t0.45\n",
      "pos\tperfect\t0.44\n",
      "pos\thighly\t0.43\n",
      "pos\tfavorite\t0.42\n",
      "pos\twonderful\t0.41\n",
      "pos\tbest\t0.41\n",
      "pos\thilarious\t0.40\n",
      "pos\tsometimes\t0.40\n",
      "pos\ttony\t0.40\n",
      "pos\tenjoyed\t0.39\n",
      "pos\ttoday\t0.39\n",
      "pos\tamazing\t0.38\n",
      "pos\thuman\t0.38\n",
      "pos\tfun\t0.38\n",
      "pos\tmad\t0.37\n",
      "pos\t8/10\t0.37\n",
      "pos\tespecially\t0.36\n",
      "pos\tlove\t0.36\n",
      "pos\tpowerful\t0.36\n",
      "\n",
      "neg\tworst\t-1.13\n",
      "neg\twaste\t-0.77\n",
      "neg\tawful\t-0.70\n",
      "neg\tboring\t-0.65\n",
      "neg\tbad\t-0.62\n",
      "neg\tworse\t-0.60\n",
      "neg\tnothing\t-0.59\n",
      "neg\tterrible\t-0.56\n",
      "neg\tunfortunately\t-0.51\n",
      "neg\twonder\t-0.50\n",
      "neg\tattempt\t-0.49\n",
      "neg\tmoney\t-0.49\n",
      "neg\thorrible\t-0.46\n",
      "neg\tinstead\t-0.46\n",
      "neg\tpoor\t-0.46\n",
      "neg\tdull\t-0.45\n",
      "neg\tavoid\t-0.43\n",
      "neg\tmess\t-0.42\n",
      "neg\tsave\t-0.41\n",
      "neg\tguess\t-0.40\n",
      "neg\tpainful\t-0.40\n",
      "neg\tsupposed\t-0.39\n",
      "neg\tidea\t-0.38\n",
      "neg\tlame\t-0.38\n",
      "neg\tunless\t-0.37\n"
     ]
    }
   ],
   "source": [
    "final_classifier.print_weights(display_k=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the most confident mistakes\n",
    "\n",
    "Finally, we can examine the reviews that the model gets most wrong. We can quantify \"most wrong\" as the misclassified reviews with the largest estimated probabilities $\\Pr(incorrect\\_label \\mid review)$. Do these examples suggest any features that might correct these mistakes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mistakes(classifier, filename, display_k=10):\n",
    "    true_labels = load_labels(filename)\n",
    "    predictions, est_probs = classifier.predict(filename, includeClassProbs=True)\n",
    "    \n",
    "    class_index = {}\n",
    "    for i, label in enumerate(classifier.model.classes_):\n",
    "        class_index[label] = i\n",
    "    \n",
    "    mistakes = []\n",
    "    with open(filename, encoding='utf-8') as reader:\n",
    "        for i, line in enumerate(reader):\n",
    "            if predictions[i] != true_labels[i]:\n",
    "                fields = line.strip().split('\\t')\n",
    "                mistake = {\n",
    "                    \"review_id\": fields[0],\n",
    "                    \"true_label\": true_labels[i],\n",
    "                    \"pred_label\": predictions[i],\n",
    "                    \"prob\": est_probs[i][class_index[predictions[i]]],\n",
    "                    \"review\": fields[2]\n",
    "                }\n",
    "                mistakes.append(mistake)\n",
    "    \n",
    "    df = pd.DataFrame(mistakes)\n",
    "    with pd.option_context('display.max_colwidth', 300):\n",
    "        display(df.nlargest(display_k, \"prob\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>true_label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>prob</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>10155</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.994444</td>\n",
       "      <td>I have read several good reviews that have defended and critised the various aspects of this film. One thing I see, over and over, is annoyance with Megan, the idealistic political scientist, trying to change the world. I loved her character. Maybe, because I am a 23 year old political science s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1293</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.989012</td>\n",
       "      <td>I am probably one of the few viewers who would not recommend this film. Thought visually stunning like all of Ang Lee's work (each still frame seems worthy of a print), I was really disappointed by the film's disjointed pace. It really was too long. The story is set in Civil War era Missouri, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>12312</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.974667</td>\n",
       "      <td>For 50 years after world war 2 the United States was in a state where key segments of the economy were dominated by military interests. At the same time, because of the draft and wars, everyone in society had served, or was connected to someone who had. This allowed for a minigenre based on the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2534</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.973286</td>\n",
       "      <td>Scientist working frantically in seclusion finds a way to locate the impact crater of a meteor carrying a new radioactive element. All (pseudo)science and breakthrough technology talks of the 1930s are right there, including the idea that radioactivity could heal any illness if properly harnesse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>9242</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.969170</td>\n",
       "      <td>It's hard to rate films like this, because do you rate it on production or just fun? I saw this film back in about 1988/89 or so when I was a boy and I'm sorry to say it started a life long fascination with ninjas. The plot is fairly dire and the acting is of course terrible, but there is a cert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2934</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.962634</td>\n",
       "      <td>Simon Pegg stars as Sidney Young, a stereotypically clumsy idiot Brit working as a celebrity journalist in this US comedy. After getting a very lucky break he starts work at the highly respected Sharps magazine run by a reliably on form Jeff Bridges in New York. It's more The Devil Wears Prada t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5486</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.960153</td>\n",
       "      <td>Yes, the cameras were in the right place at the right time. It's so interesting to see how a world leader (like Chavez) who supports the poor people in his country, can be held in such low esteem in the US. His worst \"sin\", in my opinion, is caring about those who are at the bottom of the barrel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4805</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.956129</td>\n",
       "      <td>I loved Adrianne Curry before this show. I thought she was great on Top Model and was really glad when she won. I also liked Chris Knight, he seems like a great guy. But this show just made me SICK! I'm so angry at both of them for what happened on that show. I don't care that they were differen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>10559</td>\n",
       "      <td>pos</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.951975</td>\n",
       "      <td>I haven't written a review on IMDb for the longest time, however, I felt myself compelled to write this! When looking up this movie I found one particular review which urged people NOT to see this film. Do not pay any attention to this ignorant person! NOTHING is a fantastic film, full of laughs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>4605</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.946230</td>\n",
       "      <td>This movie is written by Charlie Higson, who has before this done the \"legendary\" Fast Show and his own show based on one of Fast Show's characters (Tony the car sales man). He's also written James Bond books for kids. Actually I've seen before this only Gordon's movies that are based on Lovecra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    review_id true_label pred_label      prob  \\\n",
       "99      10155        neg        pos  0.994444   \n",
       "57       1293        neg        pos  0.989012   \n",
       "111     12312        neg        pos  0.974667   \n",
       "68       2534        pos        neg  0.973286   \n",
       "93       9242        pos        neg  0.969170   \n",
       "54       2934        neg        pos  0.962634   \n",
       "18       5486        pos        neg  0.960153   \n",
       "7        4805        neg        pos  0.956129   \n",
       "94      10559        pos        neg  0.951975   \n",
       "125      4605        neg        pos  0.946230   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                          review  \n",
       "99   I have read several good reviews that have defended and critised the various aspects of this film. One thing I see, over and over, is annoyance with Megan, the idealistic political scientist, trying to change the world. I loved her character. Maybe, because I am a 23 year old political science s...  \n",
       "57   I am probably one of the few viewers who would not recommend this film. Thought visually stunning like all of Ang Lee's work (each still frame seems worthy of a print), I was really disappointed by the film's disjointed pace. It really was too long. The story is set in Civil War era Missouri, an...  \n",
       "111  For 50 years after world war 2 the United States was in a state where key segments of the economy were dominated by military interests. At the same time, because of the draft and wars, everyone in society had served, or was connected to someone who had. This allowed for a minigenre based on the ...  \n",
       "68   Scientist working frantically in seclusion finds a way to locate the impact crater of a meteor carrying a new radioactive element. All (pseudo)science and breakthrough technology talks of the 1930s are right there, including the idea that radioactivity could heal any illness if properly harnesse...  \n",
       "93   It's hard to rate films like this, because do you rate it on production or just fun? I saw this film back in about 1988/89 or so when I was a boy and I'm sorry to say it started a life long fascination with ninjas. The plot is fairly dire and the acting is of course terrible, but there is a cert...  \n",
       "54   Simon Pegg stars as Sidney Young, a stereotypically clumsy idiot Brit working as a celebrity journalist in this US comedy. After getting a very lucky break he starts work at the highly respected Sharps magazine run by a reliably on form Jeff Bridges in New York. It's more The Devil Wears Prada t...  \n",
       "18   Yes, the cameras were in the right place at the right time. It's so interesting to see how a world leader (like Chavez) who supports the poor people in his country, can be held in such low esteem in the US. His worst \"sin\", in my opinion, is caring about those who are at the bottom of the barrel...  \n",
       "7    I loved Adrianne Curry before this show. I thought she was great on Top Model and was really glad when she won. I also liked Chris Knight, he seems like a great guy. But this show just made me SICK! I'm so angry at both of them for what happened on that show. I don't care that they were differen...  \n",
       "94   I haven't written a review on IMDb for the longest time, however, I felt myself compelled to write this! When looking up this movie I found one particular review which urged people NOT to see this film. Do not pay any attention to this ignorant person! NOTHING is a fantastic film, full of laughs...  \n",
       "125  This movie is written by Charlie Higson, who has before this done the \"legendary\" Fast Show and his own show based on one of Fast Show's characters (Tony the car sales man). He's also written James Bond books for kids. Actually I've seen before this only Gordon's movies that are based on Lovecra...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_mistakes(final_classifier, \"dev.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb835a749d7568845ea90aa49a5e948a5d84b557de2e2aaba289cd8ea94891ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
