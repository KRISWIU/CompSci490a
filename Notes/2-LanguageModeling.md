## Language Modeling ##
* Zipf's Law: å¤§æ¦‚å°±æ˜¯å•è¯å‡ºç°çš„é¢‘ç‡å’Œåœ¨é¢‘ç‡è¡¨çš„æ’åæˆåæ¯”
* Language Modeling: Model the chance of a word or text occurring
    * Pr ğ‘¡ğ‘’ğ‘¥ğ‘¡ = Pr(ğ‘¤1,ğ‘¤2, â€¦ ,ğ‘¤ğ‘›) ; Pr(ğ‘›ğ‘’ğ‘¥ğ‘¡ ğ‘¤ğ‘œğ‘Ÿğ‘‘) = Pr(ğ‘¤ğ‘›+1|ğ‘¤1,ğ‘¤2, â€¦ ,ğ‘¤ğ‘›)
    * Chain Rule: Pr ğ‘¥1ğ‘¥2ğ‘¥3ğ‘¥4 = Pr(ğ‘¥1) â‹… Pr(ğ‘¥2|ğ‘¥1) â‹… Pr(ğ‘¥3|ğ‘¥1ğ‘¥2) â‹… Pr(ğ‘¥4|ğ‘¥1ğ‘¥2ğ‘¥3)
* N-gram Language Model: 
    * Simplest: Unigram Model: P(x1x2x3x4) = P(x1) â‹… P(x2) â‹… P(x3) â‹… P(x4)
    * Biggram Model : P(x1x2x3x4) = P(x1) â‹… P(ğ‘¥2|ğ‘¥1) â‹… P(x3|x2) â‹… P(x4|x3)
    * Trigram Model 
* Evaluation Language Models:
    * **Extrinsic** evaluation: Measure model quality by **application** performance
    * **Intrinsic** evaluation: Measure model quality **independent** of applications
    * Perplexity: PP(W) = (1/Pr(w1,w2,...,wn))^(1/n)
* Additive Smoothing: avoid zero probabilities
    * Pr (wi) â‰ˆ (ğ‘“(ğ‘¤ğ‘–) + ğ›¼)/(ğ‘ + ğ‘‰)   
    * Pr (ğ‘¤ğ‘›|ğ‘¤ğ‘›âˆ’1) â‰ˆ (ğ‘“(ğ‘¤ğ‘›âˆ’1|ğ‘¤ğ‘›) + ğ›¼)/(ğ‘“(ğ‘¤ğ‘›âˆ’1) + ğ›¼ âˆ™ V)


i have lunch i eat
P(i) = 2/5
p(have|i) = 1/2
P(lunch | i have) = 1
P(i have lunch) = 1/5

i, good, bad, it
pos: good, it
neg: bad, i

i like it
P(i like it | pos) = p(i|pos) * p(like|pos) * p(it|pos)
                   = p(w1|pos) * p(w2|pos) ... 
