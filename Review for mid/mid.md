## Regular Expression ##
* Character classes
    * Disjunction of listed characters
        * [Ww ]ord = word / Word
    * Ranges
        * [A-Z ]: any upper case letter
    * Negation of class [^...... ]
    * ? * + .: 
        * ?: Optional previous char (Colou?r = Color/Colour)
        * !: 0 or more of previous char (oo!h = oh/ooooh/oooooooh)
        * +: 1 or more of previous char (o+h = oo!h)
        * .: æ›¿ä»£ä»»æ„char
* Tokenlization
    * Stages of text processing: 1. segment text into words, 2. normalization 3. æ›´ç»†èŠ‚çš„segmentation
    * Stemming & Lemmatizing
        * Stem å°±æ˜¯ playing -> play (faster)
        * Lemmatizing å°±æ˜¯ is -> be (slower)

## Language Modeling ##
* Zipf's Law: å¤§æ¦‚å°±æ˜¯å•è¯å‡ºç°çš„é¢‘ç‡å’Œåœ¨é¢‘ç‡è¡¨çš„æ’åæˆåæ¯”
* Language Modeling: Model the chance of a word or text occurring
    * Pr ğ‘¡ğ‘’ğ‘¥ğ‘¡ = Pr(ğ‘¤1,ğ‘¤2, â€¦ ,ğ‘¤ğ‘›) ; Pr(ğ‘›ğ‘’ğ‘¥ğ‘¡ ğ‘¤ğ‘œğ‘Ÿğ‘‘) = Pr(ğ‘¤ğ‘›+1|ğ‘¤1,ğ‘¤2, â€¦ ,ğ‘¤ğ‘›)
    * Chain Rule: Pr ğ‘¥1ğ‘¥2ğ‘¥3ğ‘¥4 = Pr(ğ‘¥1) â‹… Pr(ğ‘¥2|ğ‘¥1) â‹… Pr(ğ‘¥3|ğ‘¥1ğ‘¥2) â‹… Pr(ğ‘¥4|ğ‘¥1ğ‘¥2ğ‘¥3)
* N-gram Language Model: 
    * Simplest: Unigram Model: P(x1x2x3x4) = P(x1) â‹… P(x2) â‹… P(x3) â‹… P(x4)
    * Biggram Model : P(x1x2x3x4) = P(x1) â‹… P(ğ‘¥2|ğ‘¥1) â‹… P(x3|x2) â‹… P(x4|x3)
    * Trigram Model 
* Evaluation Language Models:
    * **Extrinsic** evaluation: Measure model quality by **application** performance
    * **Intrinsic** evaluation: Measure model quality **independent** of applications
    * Perplexity: PP(W) = (1/Pr(w1,w2,...,wn))^(1/n)
* Additive Smoothing: avoid zero probabilities
    * Pr (wi) â‰ˆ (ğ‘“(ğ‘¤ğ‘–) + ğ›¼)/(ğ‘ + ğ‘‰)   
    * Pr (ğ‘¤ğ‘›|ğ‘¤ğ‘›âˆ’1) â‰ˆ (ğ‘“(ğ‘¤ğ‘›âˆ’1 ğ‘¤ğ‘›) + ğ›¼)/(ğ‘“(ğ‘¤ğ‘›âˆ’1) + ğ›¼ âˆ™ V)

## Text Classification ##
* Summary:
    * Input: some text x (e.g. sentence, document)
    * Output: a label y (from some finite label set)
    * Goal: learn a mapping function f from x to y
* Sentiment Analysis ï¼ˆä½œä¸šå°±æ˜¯ï¼‰
    * Actual Task: Is the entire text of a review positive or negative?
    * 2 key components:
        * formal learning method
        * representation of the (input) data
* **Bag of Words**: Represent texts as their word counts
* **Naive Bayes**: 
    * Input Representation: Bag of words
    * Pr (ğ‘™ğ‘ğ‘ğ‘’ğ‘™|ğ‘¡ğ‘’ğ‘¥ğ‘¡)
    * å®ç°çš„æ—¶å€™ï¼Œlong documentç”¨logä¼šè§£å†³på¤ªå°çš„é—®é¢˜
    * unknwonçš„æƒ…å†µï¼Œç”¨smoothing Pr(ğ‘¤ğ‘–|ğ‘™ğ‘ğ‘ğ‘’ğ‘™) â‰ˆ (ğ‘“(ğ‘¤ğ‘–,ğ‘™ğ‘ğ‘ğ‘’ğ‘™) + ğ›¼)/(ğ‘“(ğ‘¤, ğ‘™ğ‘ğ‘ğ‘’ğ‘™) + ğ›¼ âˆ™ V)
* Evaluate Model:
    * True Table:
    * | True Positive | False Positive |
      | ----------------| ------------- |
      | False Negative | True Negative |